\documentclass{article}

\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{blkarray}
\usepackage[font=small]{caption}
\usepackage{cite}
\usepackage{graphicx}

% ---- Propositions, lemmas, defintions... ---- %

\newtheorem{algorithm}{Algorithm}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
%\newtheorem{example}{Example}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
%\newtheorem{remark}{Remark}


% ---- Special environments (examples and remarks) ---- %

\newcounter{examplecounter}
\newenvironment{example}
{\small\vspace{0.5\baselineskip}
  \refstepcounter{examplecounter}%
  \noindent\textbf{Example \arabic{examplecounter}.}%
}{\vspace{0.5\baselineskip}}

\newcounter{remarkcounter}
\newenvironment{remark}
{\small\it\vspace{0.5\baselineskip}
  \refstepcounter{remarkcounter}%
  \noindent\textbf{Remark \arabic{remarkcounter}.}%
}{\vspace{0.5\baselineskip}}


% ---- Macros ---- %

\newcommand{\smA}{\scriptstyle{A}}
\newcommand{\smC}{\scriptstyle{C}}
\newcommand{\smD}{\scriptstyle{D}}
\newcommand{\smE}{\scriptstyle{E}}
\newcommand{\smG}{\scriptstyle{G}}
\newcommand{\smI}{\scriptstyle{I}}
\newcommand{\smS}{\scriptstyle{S}}
\newcommand{\smT}{\scriptstyle{T}}
\newcommand{\smDELz}{\scriptstyle{\Delta_0}}
\newcommand{\smDELs}{\scriptstyle{\Delta_*}}


%---------------------------------------------------------------

\title{Analytic combinatorics for bioinformatics I:
seeding methods}
\author{
\textsc{Guillaume Filion and Eduard Valera Zorita} \\ [1ex]
\normalsize CRG, Barcelona
}
\date{\today}

%---------------------------------------------------------------
%---------------------------------------------------------------


\begin{document}

\maketitle

\begin{abstract}
The abstract will come later.
\end{abstract}


%---------------------------------------------------------------
%---------------------------------------------------------------

\section{Introduction}

High throughput sequencing is changing the face of biology. More data is
of course better, but recently, technical improvements have started to
outpace the progress of algorithms. When the problems are too large,
one has to replace exact algorithms by heuristics that are much faster,
but do not guarantee to return the right result. Good heuristics are all
about understanding the input data. With the right model of the data, we
can calculate the risk of not returning the right answer and adjust the
algorithm to achieve more precision or more speed. When the data is poorly
understood, heuristics may be slow or inefficient for unknown reasons.

A particular area of bioinformatics where heuristics have been in use for
a long time is the field of sequence alignment. Computing the best
alignment between two sequences is carried out by dynamic programming in
time $O(mn)$, where $m$ and $n$ are the sequence lengths. When at least
one of the sequences is long (\textit{e.g.} a genome), this is prohibitive
and heuristics are required.

The most studied heuristics for sequence alignment are called seeding
methods. In a nutshell, the idea is to search short regions of the two
sequences that are very similar and use them as candidates to anchor the
dynamic programming alignment, which is performed only locally
\textit{i.e.} between subsequences of the input. These short regions of
high similarity are called ``seeds''. The benefit of the approach is that
seeds can be found in short time. The risk is that they may not exist,
even if the input sequences are similar.

This strategy was most famously implemented in BLAST for the purpose of
finding local homology between proteins. By working out an approximate
distribution of the identity score for the seeds, the authors were able to
calibrate the BLAST heuristic very accurately in order to gain speed. The
algorithm always performs the minimum amount of work for a desired
confidence level (acceptable false negative rate).

Seeding methods are also heavily used in the mapping problem, where the
original sequence of a read must be found in a reference genome. The
dicovery of indexing methods based on the Burrows-Wheeler transform was
instrumental to develop short read mappers such as BWA and Bowtie. With
such indexes, one can know the number of occurrences of a substring in a
genome in time independent of the genome size. This yields an obvious
seeding strategy whereby all the substrings of the read are queried in the
genome. Here the seeds must have exactly the same sequence in the read and
in the genome.

The heuristic should be calibrated from the probability that a seed of
given length can be found in the read, but this problem has not been fully
solved. The answer depends on the types and frequencies of errors, which
are often context-dependent. Overall, the lack of theoretical framework to
model seeding probabilities is halting progress on this line of research.

Here we solve this problem for arbitrary error models using the powerful
theory of analytic combinatorics. This field of research was initiated by
Donlad Knuth in the early days of algorithmics, and later developed by
Robert Sedgewick and Philippe Flajolet. The theory is now mature and used
to tackle many problems outside the analysis of algorithms. However, it
has not yet been realized how useful it can be for bioinformatics.

This document is predominantly written for bioinformaticians and people
with a working knowledge of sequencing technologies and their applications.
Accordingly, the focus will be on explaining the mathematical concepts,
rather than the technological aspects. Also, our goal here is not to push
the boundaries of analytic combinatorics, but to explain how its simplest
concepts are useful to solve common problems in bioinformatics. We have
opted for simplicity, to the detriment of generality and rigor. The
results presented here are only a basic introduction to analytic
combinatorics; the field is currently much more advanced and we refer the
interested to the original literature.

\section{Analytic combinatorics}
\label{sec:anal}

\subsection{Weighted generating functions}
\label{subsec:WGF}


% ---------------  Definition of WGFs  --------------- %
\begin{definition}
\label{def:GF}
Let $\mathcal{A}$ be a set of combinatorial objects characterized by a
size and a weight. The \textbf{weighted generating function} of
$\mathcal{A}$ is defined as

\begin{equation}
\label{eq:GF1}
A(z) = \sum_{a \in A} w(a) z^{|a|},
\end{equation}

\noindent
where $|a|$ and $w(a)$ denote the size and weight of the object $a$,
respectively. This also defines a sequence $(a_k)_{k \geq 0}$ such that 

\begin{equation}
\label{eq:GF2}
A(z) = \sum_{k=0}^\infty a_k z^k.
\end{equation}

By definition $a_k = \sum_{a \in A_k}w(a)$, where $A_k$ is the class of
objects of size $k$. The number $a_k$ is called the total weight of
objects of size $k$.
\end{definition}
% ---------------------------------------------------- %


\begin{remark}
\label{rem:noweight}
If the weight of every object $a \in \mathcal{A}$ is $1$, then $A(z)$ is a
simple \emph{generating function}, and $a_k$ in expression (\ref{eq:GF2})
is the number of objects of size $k$.
\end{remark}

\begin{remark}
Typical combinatorial objects include binary trees, permutations,
derangements, multisets \textit{etc}., but here we will focus exclusively
on sequences of letters from finite alphabets. The readers in search of
concreteness can safely replace ``objects'' by ``words''.
\end{remark}

Expressions (\ref{eq:GF1}) and (\ref{eq:GF2}) are of course equivalent.
Depending on the context, we will use one or the other.

\begin{example}
\label{ex:BABA}
Let $\mathcal{A} = \{a\}$ and $\mathcal{B} = \{b\}$ be alphabets with a
single letter (of size $1$). Assume $w(a) = p$ and $w(b) = q$. The
weighted generating functions of $\mathcal{A}$ and $\mathcal{B}$ are then
$A(z) = pz$ and $B(z) = qz$, respectively.
\end{example}

The motivation for definition~\ref{def:GF} is to translate some
combinatorial operations on the objects into algebraic operations on their
weighted generating functions. Constructing complex objects from simple
ones, we construct complex weighted generating functions from simple ones.
The end goal is to count classes of objects with certain properties.
As will be clarified below, the easiest way to do this is to inspect their
weighted generating function.

Let us start with the most straightforward ways to obtain new weighted
generating functions: additions and multiplications. If $A(z)$ and $B(z)$
are the weighted generating functions of two mutually exclusive sets
$\mathcal{A}$ and $\mathcal{B}$, the weighted generating function of
$\mathcal{A} \cup \mathcal{B}$ is $A(z) + B(z)$, as appears immediately
from expression (\ref{eq:GF1}).

\begin{example}
With the definitions of example~\ref{ex:BABA}, the weighted generating
function of the alphabet $\mathcal{A} \cup \mathcal{B}= \{a,b\}$ is $pz +
qz = A(z) + B(z)$.
\end{example}

Size and weight can be defined for pairs of objects in $\mathcal{A} \times
\mathcal{B}$ as $|(a,b)| = |a| + |b|$ and $w(a,b) = w(a)w(b)$. In other
words the sizes are added and the weights are multiplied.  With this
convention, the weighted generating function of the Cartesian product
$\mathcal{A} \times \mathcal{B}$ is $A(z)B(z)$. This simply follows from
expression (\ref{eq:GF1}) and

\begin{equation*}
A(z)B(z) =
\sum_{a\in \mathcal{A}}w(a)z^{|a|} \sum_{b\in \mathcal{B}}w(b)z^{|b|}
= \sum_{(a,b) \in \mathcal{A} \times \mathcal{B}} w(a)w(b)z^{|a|+|b|}.
\end{equation*}

\begin{example}
With the definitions of example~\ref{ex:BABA}, $\mathcal{A}^2 = \{(a,a)\}$
contains a single pair of letters, with size $2$ and weight $p^2$. Its
weighted generating function is $(pz)^2 = A(z)^2$.
\end{example}


\begin{example}
Still with the definitions of example~\ref{ex:BABA}, $(\mathcal{A} \cup
\mathcal{B})^2$ contains the four pairs of letters $(a,a)$, $(a,b)$,
$(b,a)$ and $ (b,b)$. They have size $2$ and respective weight $p^2$,
$pq$, $qp$, and $q^2$, so the weighted generating function of
$(\mathcal{A} \cup \mathcal{B})^2$ is $(p^2+2pq+q^2)z^2 =
\big(A(z)+B(z)\big)^2$.
\end{example}


We can further extend the definition of size and weight to any finite
Cartesian product in the same way. The sizes are always added and the
weights are always multiplied. The generating function of a cartesian
product then comes as the product of their generating functions. This
allows us to construct finite sequences of objects.

\begin{example}
\label{ex:sequences}
With the definitions of example~\ref{ex:BABA}, $\mathcal{A}^k = \{(a, a,
\ldots, a)\}$ contains a single $k$-tuple of letters, with size $k$ and
weight $p^k$, so its weighted generating function is $(pz)^k$. Since the
sets $\mathcal{A}, \mathcal{A}^2, \mathcal{A}^3,\ldots$ are mutually
exclusive, the weighted generating function of their union is

\begin{equation*}
pz + (pz)^2 + (pz)^3 + \ldots
\end{equation*}

This infinite Taylor series can be expressed as a simple function. For any
$k \geq 1$, $(1-pz) \big(pz + (pz)^2 + \ldots + (pz)^k \big) =
pz-(pz)^{k+1}$.  If $|z| < 1/p$, the term $(pz)^{k+1}$ vanishes as $k$
increases. So the weighted generating function is defined for $|z| < 1/p$
and is equal to

\begin{equation*}
pz + (pz)^2 + (pz)^3 + \ldots = \frac{pz}{1-pz}.
\end{equation*}
\end{example}

Example~\ref{ex:sequences} can be generalized.

\begin{proposition}
Let $\mathcal{A}$ be a set with weighted generating function $A(z)$. The
weighted generating function of $\mathcal{A}^+ = \cup_{k=1}^\infty
\mathcal{A}^k$, called the set of \textbf{non-empty sequences} of elements
of $\mathcal{A}$ is defined for $|A(z)| < 1$ and is equal to

\begin{equation*}
\frac{A(z)}{1-A(z)}.
\end{equation*}
\end{proposition}

\begin{proof}
For $k \geq 1$, the weighted generating function of $\mathcal{A}^k$ is
$A(z)^k$. Since the sets $\mathcal{A}^k$ are mutually exclusive, the
weighted generating function of their union is $A(z) + A(z)^2 + \ldots =
A(z) / (1-A(z))$, provided $|A(z)| < 1$.
\end{proof}

We introduce the \textbf{empty object} $\varepsilon$, which has size $0$
and weight 1. Its weighted generating function is thus $1$. By convention,
we define the zeroth power of a combinatorial set $\mathcal{A}$ as
$\mathcal{A}^0 = \{\varepsilon\}$. With this definition, we can state a
variant of the previous proposition.

\begin{proposition}
\label{th:sequences}
Let $\mathcal{A}$ be a set with weighted generating function $A(z)$. The
weighted generating function of $\mathcal{A}^* = \cup_{k=0}^\infty
\mathcal{A}^k$, called the set of \textbf{sequences} of elements of
$\mathcal{A}$ is defined for $|A(z)| < 1$ and is equal to

\begin{equation*}
\frac{1}{1-A(z)}.
\end{equation*}
\end{proposition}

\begin{proof}
$\mathcal{A}^* = \{\varepsilon\} \cup \mathcal{A}^+$. By proposition
\ref{th:sequences}, the weighted generating function is $1 + A(z)(1-A(z))
= 1/(1-A(z))$, provided $|A(z)| < 1$.
\end{proof}

\begin{remark}
In what follows we will not state explicitly the conditions of convergence
of weighted generating functions. We will always assume that $|z|$ is
lower than the radius of convergence of the given expressions.
\end{remark}

\begin{remark}
These expressions are not defined for $A(z) = 1$, \textit{i.e.} when
$\mathcal{A}$ contains only the empty object. In other words, one cannot
construct sequences of empty ojects.
\end{remark}

It is important to insist that the expression ``sequences of'' refers to
the set $\mathcal{A}^*$ and not $\mathcal{A}^+$, \textit{i.e.} the empty
object $\varepsilon$ is a sequence of anything. For clarity, we will
systematically use the expression ``non-empty sequences of'' when
$\varepsilon$ is excluded.

\begin{example}
\label{ex:AUB+}
Following the definitions of example~\ref{ex:BABA}, $(\mathcal{A} \cup
\mathcal{B})^*$ is the set of sequences of $a$'s and $b$'s. By
proposition~\ref{th:sequences}, the weighted generating function of this
set is

\begin{equation*}
\frac{1}{1-(p+q)z}.
\end{equation*}

The function $1 / (1-(p+q)z)$ is just a compact representation of the
infinite series $1, (p+q), (p+q)^2, (p+q)^3, \ldots$ It thus carries all
the information about the total weights of the sequences of any size. If
$p = q = 1$, \textit{i.e.} if we count the total amount of sequences with
simple generating functions (see remark~\ref{rem:noweight}), we obtain
$1/(1-2z) = 1+ 2z + 4z^2 + 8z^3 + 16z^4 + \ldots$ meaning that there are
$2^k$ sequences of size $k$. If $p$ and $q$ are the respective
probabilities of ocurrence of $a$ and $b$, with $p + q \leq 1$, then the
equality $1 / (1-(p+q)z) = 1+ (p+q)z + (p+q)^2z^2 + (p+q)^3z^3 + \ldots$
means that the probability that a sequence of size $k$ contains only $a$'s
and $b$'s is $(p+q)^k$.
\end{example}

These trivial examples are better solved by intuition. However, we will
soon see that analytic combinatorics allows us to solve a wide range of
problems where intuition does not help.



%%%%%%%%%%%%%%%%% Transfer matrices %%%%%%%%%%%%%%%%%

\subsection{Sequences and transfer matrices}

In many combinatorial applications, one needs to count the sequences where
a pattern does not occur, or where some symbols may not follow each other.
A convenient way to find the weighted generating functions of such
sequences is to encode this information in so-called ``transfer
matrices''. We will illustrate the process with an example from biology,
before introducing the formal definitions.

In many animal genomes, DNA methylation can only occur on the dinucleotide
\texttt{CG}. Because of this property, it is interesting to count the
sequences that have no \texttt{CG}. Sequences without \texttt{CG} can be
thought of as walks on a directed graph with restricted transitions.
Nucleotides are represtend as nodes, and an edge indicates that the
nucleotide at the tip of the edge can follow the nucleotide at the base of
the edge. Sequences without \texttt{CG} correspond to a complete graph
where the edge from \texttt{C} to \texttt{G} was removed (see
figure~\ref{fig:CG_transtions}).

\begin{figure}[h]
\centering
\includegraphics[scale=0.9]{CG_transitions.pdf}
\caption{\textbf{Graph with restricted transitions}.
A sequence without \texttt{CG} can be seen as a walk on the graph above.
All the transitions are allowed, except \texttt{C} to \texttt{G}.}
\label{fig:CG_transtions}
\end{figure}

The same graph can also be represted as an adjacency matrix $M$, where a
$1$ at position $(i,j)$ indicates that there is an edge from node $i$ to
node $j$, and a $0$ indicates that there is no such edge. For instance,
the adjacency matrix of the graph of figure~\ref{fig:CG_transtions} is

\begin{equation*}
M = 
\begin{blockarray}{ccccc}
     & \smA & \smC & \smG & \smT \\
\begin{block}{c[cccc]}
\smA & 1 & 1 & 1 & 1 \\
\smC & 1 & 1 & 0 & 1 \\
\smG & 1 & 1 & 1 & 1 \\
\smT & 1 & 1 & 1 & 1 \\
\end{block}
\end{blockarray}.
\end{equation*}

The main advantage of the adjacency matrix representation is that the
powers of $M$ give an analytical way to count the walks on a graph. The
entry of $M^n$ at position $(i,j)$ is the number of walks of $n$ steps
that start with node $i$ and end with node $j$. So we can count
sequences without \texttt{CG} by computing the successive powers of the
adjacency matrix $M$ above.

The same idea can be used to find the weighted generating function of
sequences of combinatorial objects. We can replace the entries of $M$ by
generating functions to obtain a matrix $M(z)$ whose successive owers give
the generating fuctions of finite sequences of objects.

\begin{definition}
\label{def:transfermat}
Let $(\mathcal{A}_i)_{1 \leq i \leq m}$ be classes of combinatorial
objects arranged in finite sequences. A \textbf{transfer matrix} is a
$m\times m$ square matrix $M(z)$ such that for any sequence with weighted
generating function $F(z)$ ending with an object from $\mathcal{A}_i$,
sequences obtained by appending an object from $\mathcal{A}_j$ have
weighted generating function $F(z)M(z)_{i,j}$.
\end{definition}

\begin{example}
Let $\mathcal{A} = \{a\}$ be an alphabet with a single letter. Denote its
weighted generating function $pz$. A finite sequence of objects from
$\mathcal{A}$ is an object from $\mathcal{A}^k$ with weighted generating
function $(pz)^k$ for some $k \geq 0$. Appending an object from
$\mathcal{A}$, we obtain an object from $\mathcal{A}^{k+1}$ with weighted
generating function $(pz)^{k+1}$. Therefore, the transfer matrix of
sequences of objects from $\mathcal{A}$ is $M(z) = [pz]$.
\end{example}

\begin{example}
\label{ex:transmat_ab}
Let $\mathcal{A} = \{a\}$ and $\mathcal{B} = \{b\}$ be alphabets with a
single letter. Denote their weighted generating functions $pz$ and $qz$,
respectively. Let $\mathcal{S}$ be the class of sequences of objects from
either $\mathcal{A}$ or $\mathcal{B}$. It is easy to see that elements of
$\mathcal{S}$ have weighted generating functions that are products of $pz$
and $qz$. Appending an element of $\mathcal{A}$ or $\mathcal{B}$ amounts
to multiplying the weighted generating function by $pz$ or $qz$,
respectively. The transfer matrix of sequences of $\mathcal{S}$ is thus

\begin{equation*}
M(z) =
\begin{blockarray}{ccc}
     & \mathcal{A} & \mathcal{B} \\
\begin{block}{c[cc]}
\mathcal{A} & pz & qz \\
\mathcal{B} & pz & qz \\
\end{block}
\end{blockarray}.
\end{equation*}
\end{example}

\begin{example}
As in example~\ref{ex:transmat_ab}, let $\mathcal{A} = \{a\}$ and
$\mathcal{B} = \{b\}$ be alphabets with a single letter and denote their
weighted generating functions $pz$ and $qz$, respectively. This time, let
$\mathcal{S}$ be the class of sequences of objects from either
$\mathcal{A}$ or $\mathcal{B}$ such that two elements of $\mathcal{A}$
never follow each other. In this case, appending an element of
$\mathcal{A}$ to a sequence ending with an element of $\mathcal{A}$ is
impossible and so yields a weighted generating function equal to $0$.
Based on the previous example, the transfer matrix of sequences in
$\mathcal{S}$ is thus

\begin{equation*}
M(z) =
\begin{blockarray}{ccc}
     & \mathcal{A} & \mathcal{B} \\
\begin{block}{c[cc]}
\mathcal{A} & 0  & qz \\
\mathcal{B} & pz & qz \\
\end{block}
\end{blockarray}.
\end{equation*}
\end{example}

This background in mind, we now introduce transfer matrices on the problem
at hand to find the weighted generating function of sequences without
\texttt{CG}.  Assume that \texttt{G}s and \texttt{G}s both occur with
frequency $p/2$ and that \texttt{A}s and \texttt{T}s occur with frequency
$q/2$, where $q = 1-p$. With obvious notations we see that $C(z) = G(z) =
pz/2$ and that $A(z) = T(z) = qz/2$.

\begin{example}
\label{ex:CGmat}
Ordering the weighted generating functions of nucleotides alphabetically
as $(A(z), C(z), G(z), T(z))$, the transfer matrix of sequences without
\texttt{CG} can be written as

\begin{equation}
\label{eq:Mz_CG}
M(z) =
\begin{blockarray}{ccccc}
     & \smA & \smC & \smG & \smT \\
\begin{block}{c[cccc]}
\smA & A(z) & C(z) & G(z) & T(z) \\
\smC & A(z) & C(z) &  0   & T(z) \\
\smG & A(z) & C(z) & G(z) & T(z) \\
\smT & A(z) & C(z) & G(z) & T(z) \\
\end{block}
\end{blockarray}
= z/2 \left[
\begin{matrix}
q & p & p & q \\
q & p & 0 & q \\
q & p & p & q \\
q & p & p & q
\end{matrix}
\right].
\end{equation}
\end{example}


The following two propositions show why transfer matrices are useful to
find the weighted generating function of combinatorial sequences.

\begin{proposition}
\label{th:transfermatrices}
Let $(\mathcal{A}_i)_{1 \leq i \leq m}$ be classes of combinatorial
objects with generating functions $A_i(z)$. Let $M(z)$ be the transfer
matrix of some class of sequences of objects from $(\mathcal{A}_i)_{1 \leq
i \leq m}$. The weighted generating function of sequences of $n+1$
objects, starting with an object from $\mathcal{A}_i$ and ending with an
object from $\mathcal{A}_j$ is

\begin{equation}
\label{eq:mznij}
A_i(z) \cdot \left( M(z)^n \right)_{i,j}.
\end{equation}

\end{proposition}

\begin{proof}
Proceed by induction. For $n = 0$, the sequences have only one item.
Expression (\ref{eq:mznij}) is equal to $A_i(z)$ if $i = j$ and $0$
otherwise, which is the generating function of sequences of one item
starting with an object from $\mathcal{A}_i$ and ending with an object
from $\mathcal{A}_j$.

Assume that (\ref{eq:mznij}) holds for $n \geq 0$. A sequence of $n+2$
items starting with an object from $\mathcal{A}_i$ and ending with an
object from $\mathcal{A}_j$ is a sequence of $n+1$ items starting with an
object from $\mathcal{A}_i$ and ending with an object from some
$\mathcal{A}_k$, followed by an object from $\mathcal{A}_j$. Using the
recurrence hypothesis, the weighted generating function of sequences of
$n+1$ items starting with an object from $\mathcal{A}_i$ and ending with
an object from $\mathcal{A}_k$ is $A_i(z) \cdot (M(z)^n)_{i,k}$. Using the
deifinition of transfer matrices, $A_i(z) \cdot (M(z)^n)_{i,k} \cdot
M(z)_{k,j}$ is the weighted generating function of sequences of $n+2$
objects, starting with an object from $\mathcal{A}_i$ and ending with an
object from $\mathcal{A}_k$ followed by an object from $\mathcal{A}_j$.

Now summing on all $\mathcal{A}_k$, we see that the weighted generating
function of sequences of $n+2$ objects, starting with an element from
$\mathcal{A}_i$ and ending with an element from $\mathcal{A}_j$ is

\begin{equation*}
\sum_{k = 1}^m A_i(z)\left( M(z)^n \right)_{i,k} M(z)_{k,j} 
 = A_i(z) \left( M(z)^{n+1} \right)_{i,j},
\end{equation*}

\noindent
which concludes the induction.
\end{proof}


% --- The corollary --- %

\begin{corollary}
The weighted generating function of sequences of items starting with an
object from $\mathcal{A}_i$ and ending with an object from $\mathcal{A}_j$
is

\begin{equation}
\label{eq:I-Mz}
A_i(z) \cdot \left( I - M(z) \right)^{-1}_{i,j},
\end{equation}

\noindent
provided the eigenvalues of $M(z)$ all have \textit{modulus} less than
$1$.
\end{corollary}

\begin{proof}
Summing (\ref{eq:mznij}) on all $n$, the weighted generating function of
sequences of objects starting with an object from $\mathcal{A}_i$ and
ending with an object from $\mathcal{A}_j$ appears as

\begin{equation*}
\sum_{n=0}^\infty A_i(z) \cdot \left(M(z)^n \right)_{i,j} =
 A_i(z) \cdot \left( \sum_{n=0}^\infty M(z)^n\right)_{i,j}.
\end{equation*}

Observe that $(I-M(z)) \cdot (I+M(z)+M(z)^2+ \ldots + M(z)^n) =
I-M(z)^{n+1}$. If all the eigenvalues of $M(z)$ have \textit{modulus} less
than $1$, the right hand size converges to the identity matrix $I$ as $n$
goes to infinity. This translates to

\begin{equation*}
\sum_{n=0}^\infty M(z)^n = (I-M(z))^{-1},
\end{equation*}

\noindent
concluding the proof that the weighted generating function is as shown in
(\ref{eq:I-Mz}).
\end{proof}


\begin{example}
\label{ex:CGmat2}
Continuing example~\ref{ex:CGmat}, the weighted generating function of
sequences without \texttt{CG} that start with a \texttt{C} and end with a
\texttt{G} is

\begin{equation*}
C(z) \cdot (I-M(z))^{-1}_{2,3}.
\end{equation*}

The inverse of $I-M(z)$ is equal to 

\begin{equation*}
\frac{1}{\lambda(z)} \left[
\begin{matrix}
(pz)^2+2qz+4   & 2pz        & pz(pz-2)   & 2qz                \\
-pqz^2+2qz     & -2(1+q)z+4 & 2pqz^2     & -pqz^2+2qz         \\
2qz            & 2pz        & -2(1+q)+4  & 2qz                \\
2qz            & 2pz        & pz(1-pz)   & (pz)^2 - 2(p+1) +4
\end{matrix}
\right]
\end{equation*}

\noindent
where $\lambda(z) = (pz)^2 - 4z + 4$ is the determinant of $I-M(z)$. Now
picking the entry at coordinates $(2,3)$, and multiplying by $C(z) =
pz/2$, we find that the weighted generating function is

\begin{equation*}
\frac{pz}{2} \cdot \frac{2pqz^2}{(pz)^2 - 4z + 4}
= \frac{p^2qz^3}{(pz)^2 - 4z + 4}.
\end{equation*}
\end{example}

Finally, we introduce the ``head-body-tail'' decomposition, which allows
us to find the weighted generating functions of general combinatorial
sequences.

\begin{proposition}
\label{th:TM2WGF}
The weighted generating function of all possible sequences described by
the transfer matrix $M(z)$ is 

\begin{equation}
1 + H \cdot B(z) \cdot T.
\end{equation}

BLAH BLAH.
\end{proposition}

\begin{proof}
Applying (\ref{eq:I-Mz}) and summing for all possible start and end cases
gives $U(z) \cdot (I-M(z))^{-1} \cdot V$. We need to add $1$ to account
for the empty sequence.
\end{proof}

\begin{example}
Continuing example~\ref{ex:CGmat2}, sequences without \texttt{CG} may
start with all the nucleotides, so $U(z)$ is the row vector $(A(z), C(z),
G(z), T(z))$. Likewise, they may end with all the nucleotides, so $V$ is
the column vector $(1,1,1,1)$. The weighted generating function of all
possible sequences (including the empty one) without \texttt{CG} is

\begin{equation}
\label{eq:WGFnoCG}
F(z) = \frac{4}{4-4z+(pz)^2}.
\end{equation}
\end{example}

Proposition~\ref{th:transfermatrices} allows us to construct
elaborate weighted generating functions. By expression (\ref{eq:GF1}) from
the definition, the coefficient of $z^k$ in the Taylor expansion of
(\ref{eq:WGFnoCG}) is the sum of the weights of all the sequences of size
$k$ that contain no \texttt{CG}. Due to our normalization, this is also
the probability that a sequence of size $k$ contains no \texttt{CG}.
It would be very useful if we could extract those coefficients from
(\ref{eq:WGFnoCG}) directly. The next section will give a generic method
to do so.



%%%%%%%%%%%%% The crown jewel proposition %%%%%%%%%%%%%

\subsection{Asymptotic estimates}

It is not always possible to extract the exact coefficients of a
generating function, but one of the crown jewels of analytic combinatorics
is that we can very accurately approximate them.

\begin{proposition}
\label{th:ass}
If a weighted generating function $A(z)$ is the ratio of two polynomials
$P(z)/Q(z)$, and the roots of $Q$ are simple, then the coefficient of
$z^k$ in its series expansion is asymptotically equivalent to

\begin{equation}
\label{eq:ass}
-\frac{P(z_1)}{Q'(z_1)}\frac{1}{z_1^{k+1}},
\end{equation}

\noindent
where $z_1$ is the root of $Q$ with smallest \textit{modulus}.
\end{proposition}

The roots of $Q$ are called the ``singularities'' of the weighted
generating function $A$. They are values where the function is not
defined. When they correspond to simple roots of $Q$ (\textit{i.e.} with
multiplicity 1), they also referred to as ``simple poles'' of $A$.

Proposition~\ref{th:ass} says that the asymptotic growth of the
coefficients of the series expansion of $A(z)$ is dictated by the
singularity of smallest \textit{modulus}, also known as the ``dominant
singularity'' of $A$. We will first state a lemma that will be important
to improve the asymptotic approximation of the coefficients.

\begin{lemma}
\label{lemma:poles}
For $|z| < a$ we have

\begin{equation}
\label{eq:poles}
\frac{1}{1-z/a} = \sum_{k=0}^\infty \frac{z^k}{a^k}.
\end{equation}
\end{lemma}

\begin{proof}
Proceed as in example~\ref{ex:sequences}, replacing $p$ by $1/a$.
\end{proof}

We now prove proposition~\ref{th:ass}.

\begin{proof}
Let $z_1, z_2, \ldots, z_n$ be the complex roots of $Q$, sorted by
increasing order of \textit{modulus}. Since $Q$ has only simple roots,
there exists constants $\beta_1, \ldots, \beta_n$ such that the partial
fraction decomposition of the rational function $P(z)/Q(z)$ can be written
as

\begin{equation}
R(z) + \sum_{j=1}^n \frac{\beta_j}{z-z_j} =
R(z) -\sum_{j=1}^n \frac{\beta_j/z_j}{1-z/z_j}.
\end{equation}

Here $R(z)$ is a polynomial that is nonzero ony if the degree of $P$ is
higher than the degree of $Q$. Either way, the coefficient of $z^k$ in
$R(z)$ is $0$ for $k$ higher than the degree of $R$, so those coefficients
do not contribute to the asymptotics. We can thus assume $R(z) = 0$
without loss of generatlity. Using lemma~\ref{lemma:poles}, we see that
the coefficient of $z^k$ in the series expansion of $F(z)$ is equal to

\begin{equation}
\label{eq:fullass}
-\sum_{j=1}^n \frac{\beta_j}{z_j^{k+1}}.
\end{equation}

Since $z_1$ is the root with smallest \textit{modulus}, the sum is
asymptotically equivalent to

\begin{equation*}
-\frac{\beta_1}{z_1^{k+1}}.
\end{equation*}

To find the value of $\beta_1$, we factorize $Q(z)$ as
$(z-z_1)Q_1(z)$, which is possible because $z_1$ is a root of $Q$,
and we write

\begin{equation*}
\frac{P(z)}{Q(z)} =
\frac{P(z)}{(z-z_1)Q_1(z)} = \frac{\beta_1}{z-z_1} +
\varepsilon(z),
\end{equation*}

\noindent
where $\varepsilon(z)$ is the remainder of the partial fraction
decomposition. Multiplying both sides of the equality by $(z-z_1)$ and
setting $z = z_1$, we obtain the expression $\beta_1 = P(z_1) / Q_1(z_1)$.
Differentiating the definint expression $Q(z) = (z-z_1)Q_1(z)$ shows that
$Q'(z_1) = Q_1(z_1)$, and thus that $\beta_1 = P(z_1) / Q'(z_1)$, which
concludes the proof.
\end{proof}

\begin{remark}
Expression (\ref{eq:fullass}) is not an approximation, it is the exact
value of the coefficient. By keeping more than one term, we can obtain
more accurate estimates, and by keeping all the terms we obtain the exact
number.
\end{remark}

\begin{remark}
The convergence to the asymptotic estimate is exponential. To see this,
divide the exact expression (\ref{eq:fullass}) by its leading term
$-\beta_1/z_1^{k+1}$ and obtain

\begin{equation*}
\frac{-\sum_{j=1}^n \beta_j/z_j^{k+1}}{-\beta_1/z_1^{k+1}} =
1 + \sum_{j=2}^n
\frac{\beta_j}{\beta_1} \left( \frac{z_1}{z_j} \right)^{k+1}.
\end{equation*}

Since $|z_1| < |z_j|$ for $2 \leq j \leq n$, the error terms are
$O(|z_1/z_2|^k)$ and they decrease exponentially fast as $k$ increases.
\end{remark}

\begin{remark}
Proposition~\ref{th:ass} does not hold if $z_1$ is not a simple pole.
The proof can be beneralized, but the resulting asymptotic formula is
different. Proposition~\ref{th:ass2} in section~\ref{sec:avsub} shows the
coefficient asymptotics for poles of second order.
\end{remark}

\begin{example}
Recall from example~\ref{ex:CGmat2} that the weighted generating function
of sequences without \texttt{CG} is

\begin{equation*}
F(z) = \frac{4}{4-4z+(pz)^2}.
\end{equation*}

Here $Q(z) = 4-4z+(pz)^2$ has two distinct roots, $z_1 =
2(1-\sqrt{1-p^2})/p^2$ and $z_2 = 2(1+\sqrt{1-p^2})/p^2$. Since $Q'(z) =
2p^2z-4$, the coefficient of $z^k$ is the Taylor expansion of $F(z)$ is
asymptotically equivalent to

\begin{equation*}
\begin{split}
-\frac{P(z_1)}{Q'(z_1)}\frac{1}{z_1^{k+1}} &=
\frac{1}{\sqrt{1-p^2}}
\left( \frac{p^2/2}{1-\sqrt{1-p^2}} \right)^{k+1} \\
&= \frac{1}{\sqrt{1-p^2}} \left(\frac{1+\sqrt{1-p^2}}{2} \right)^{k+1}.
\end{split}
\end{equation*}

In this case we can also obtain the exact answer by using the second root.
The probability that a sequence of size $k$ contains no \texttt{CG} is
exactly equal to

\begin{equation*}
\begin{split}
-\frac{P(z_1)}{Q'(z_1)}\frac{1}{z_1^{k+1}}
&-\frac{P(z_2)}{Q'(z_2)}\frac{1}{z_2^{k+1}} \\
&=
\frac{1}{\sqrt{1-p^2}} \left[
\left( \frac{p^2/2}{1-\sqrt{1-p^2}} \right)^{k+1}
- \left( \frac{p^2/2}{1+\sqrt{1-p^2}} \right)^{k+1}\right] \\
&= \frac{1}{\sqrt{1-p^2}} \left[
\left(\frac{1+\sqrt{1-p^2}}{2} \right)^{k+1} -
\left(\frac{1-\sqrt{1-p^2}}{2} \right)^{k+1} \right].
\end{split}
\end{equation*}

In case all the nucleotides have the same frequency, $p=1/2$ and the
probability is approximately equal to $(0.9330127^{k+1} - 0.0669873^{k+1})
/ 1.154701$. From $k=5$, the second term is more than one million times
smaller than the first, which shows how fast it can be neglected.
\end{example}

Things do not always turn out that simple, but this example illustrates
how analytic combinatorics can offer simple, yet not intuitive solutions.
Actually, it is not clear how one may come to the exact solution with
another approach. This example also illustrates how close the approximate
solution typically is to the exact one.

The purpose of this example is only to expose the analytic combinatorics
approach, which we summarize as follows: $(i)$ define simple objects
associated to simple generating functions, $(ii)$ combine these objects
into more complex structures, $(iii)$ translate those combinations into
more complex generating functions, and $(iv)$ use analytic transfer
theorems such as proposition~\ref{th:ass} to extract coefficients
asymptotics.










%%%%%%%%%%%%% Seeding problem begins %%%%%%%%%%%%%

\section{Exact seeding}

From the eperimental point of view, a sequencing read is the result of an
assay on some molecule of a nucleic acid. The output of the assay is the
decoded sequence of monomers that compose the molecule. From the formal
point of view, a read is a finite sequence of symbols.

The focus here is not the nucleotide sequence \textit{per se}, but whether
the symbols are correct. We picture a read as a sequence of symbols
representing the different sequencing error, namely substitutions,
deletions and insertions, plus a symbol for no error\footnote{How do we
know the type of error?}. Figure~\ref{fig:sketchseed} shows the structure
of reads for the seeding problem.

\begin{figure}[h]
\centering
\includegraphics[scale=0.88]{sketch_seeding.pdf}
\caption{\textbf{Structure of reads}. Here we consider sequencing reads
that can have any type of error (insertions, deletions or substitutions).
The reads have size $k$, and the errors are represented as grey squares. A
read is composed of error-free intervals and error-only intervals. Note
that a deletion is an error-only interval of size $0$, so two error-free
intervals can be contiguous (between position $3$ and $4$ in this
example). However error-free intervals have size at least $1$, so two
error-only intervals cannot be contiguous. \textbf{MAKE IT CLEAR THAT
THIS IS NOT A SIMPLIFYING ASSUMPTION!}}
\label{fig:sketchseed}
\end{figure}

Error-free stretches are particularly important because they faithfully
represent the molecule that was sequenced. If they are long, it is
relatively easy to identify the molecule based on the decoded sequence
because the information content is high, but if they are short, it usually
proves more difficult. For instance, modern mapping algorithms are based
on exact searches, using either hash tables or indexes based on the
Burrows-Wheeler transform, so error-free stretches directly influence the
performance of the alrogithm.

In what follows, the feature of interest is the size of the longest
error-free stretch. To facilitate the discussion, we need to introduce
some terms that will be used throughout.

\begin{definition}
\label{def:error-free-interval}
An \textbf{error-free interval} is a non-empty sequence of correct calls
that cannot be extended left or right.
\end{definition}

In other words, an error-free interval contains no error, and it is
flanked by either an error, or by the end of the read. It is important to
highlight that error-free intervals should really be called ``maximal
error-free intervals''. We dropped ``maximal'' for simplicity because we
have no use for non maximal error-free intervals. In the read shown in
figure~\ref{fig:sketchseed}, the leftmost error-free interval has size
$3$, the next has size $1$, the next has size $3$ and the next has size
$4$.

\begin{definition}
\label{def:seed}
An \textbf{exact $d$-seed} is an error-free interval of size at least $d$.
\end{definition}

In what follows, we will refer to a $d$-seed as simply a ``seed''
whenever the value of $d$ is clear from the context or irrelevant.

Our goal here is to estimate the probability that a read contains a seed.
This obviously depends on the size of the read and on the error rate, but
also on the types of errors. Below we develop a general approach to answer
this question building on the tools developed in
section~\ref{sec:anal}.

The strategy is to first exhibit the weighted generating functions of
reads that contain no seed and then use proposition~\ref{th:ass} to
approximate their probability of occurrence.

Observe that a read is a sequence of error-free intervals, interspersed
with errors. This observation motivates us to use transfer matrices to
express the weighted generating function of reads in terms of $F(z)$, the
weighted generating function of error-free intervals. With an expression
where $F(z)$ appears explicitly, we can then truncate it as $F_d(z)$, the
weighted generating function of error-free intervals of size smaller than
$d$. This the corresponds to sequences of ``small'' error-free intervals
interspersed with errors, \textit{i.e.} to reads without
seed\footnote{Clarify that we do not want $1-F_d(z)$, because $F_d(z)$ is
not a probability function.}.

To introduce the concepts progressively, we first describe simplified
models where some types of errors are disallowed. We then describe the
more complex models.





%%%%%%%%%%%%%%%%%% Substitutions only %%%%%%%%%%%%%%%%%%%

\subsection{Substitutions only}
\label{sec:substitutions}

One of the simplest and yet useful models is to assume that errors consist
of substitutions only, and that they occur with the same probability $p$
for every nucleotide. This describes reasonably well the error model of
the Illumina platforms (where $p$ is around $0.01$).

A read can be thought of as a walk on the graph shown in
figure~\ref{fig:subonly}. Each nucleotide is represented as a node that
can be either a match or a substitution. The weights on the edges indicate
the probabilities of transitions between the states. In this model, the
transitions are simply equal to the proportions of the different states
(\textit{i.e.} the error rate).

\begin{figure}[h]
\centering
\includegraphics[scale=0.9]{substitutions_only.pdf}
\caption{\textbf{Error model with substitutions only}. 
The substitution rate $p$ is assumed to be constant throughout the read.
Every decoded nucleotide is correct with probability $q = 1-p$.}
\label{fig:subonly}
\end{figure}

With these assumptions, the weighted generating function of each mismatch
is $pz$ and that of correct call is $qz$, where $q=1-p$. Since error-free
intervals are non-empty sequences of correct calls, their weighted
generating function is simply

\begin{equation}
\label{eq:Fsub}
F(z) = qz + (qz)^2 + (qz)^3 + \ldots = \frac{qz}{1-qz}.
\end{equation}

An error-free interval cannot follow another error-free interval,
otherwise one of them could be extended. So an error-free interval must be
followed by a substitution. On the other hand, a substitution can be
followed by either an error-free interval or another substitution. So the
transfer matrix for the states ``match'' and ``substitution'' is

\begin{equation*}
M(z) = 
\begin{blockarray}{ccc}
       & \smDELz & \smS \\
\begin{block}{c[cc]}
\smDELz & 0    & pz \\
\smS    & F(z) & pz \\
\end{block}
\end{blockarray}.
\end{equation*}


To find the weighted generating function of reads from
proposition~\ref{th:TM2WGF}, we need to find the inverse of $I-M(z)$.
Since $M(z)$ is a $2 \times 2$ matrix, this is straightforward and we
obtain

\begin{equation*}
M_*(z) = (I-M(z))^{-1}=
\frac{1}{\lambda(z)}
\left(
\begin{matrix}
1-pz & pz   \\
F(z) & 1
\end{matrix}
\right)
\end{equation*}

\noindent
where $\lambda(z) = 1-pz(1+F(z))$ is the determinant of $I-M(z)$. Applying
proposition~\ref{th:TM2WGF} with $U(z)$ equal to the row vector $(F(z),
pz)$ and $V$ the column vector equal to $(1,1)$, the weighted generating
function of reads is

\begin{equation}
\label{eq:Rsub}
R(z) = 1 + U(z) \cdot M_*(z) \cdot V = 
\frac{1+F(z)}{1-pz(1+F(z))} = \frac{1}{1-z}.
\end{equation}

Since $1/(1-z) = 1+z+z^2 + \ldots$, this means that the total weight of
reads of size $k$ is equal to $1$ for every finite $k$. This is convenient
because the probability that a read is seedless is the total weight of
seedless reads divided by the total weight of reads, which in this case is
simply equal to the total weight of seedless reads.

To find the weighted generating function of seedless reads, we need to
limit error-free intervals to a maximum size of $d-1$, \textit{i.e.} to
replace $F(z)$ by its truncation $F_d(z) = qz + (qz)^2 + \ldots
+ (qz)^{d-1}$. We obtain

\begin{equation}
\label{eq:Sp}
S(z) = \frac{1+F_d(z)}{1-pz\big( 1+F_d(z) \big)} =
\frac{1+qz + \ldots + (qz)^{d-1}}{1-pz \big(1+qz + \ldots +
(qz)^{d-1} \big)}.
\end{equation}

The task is now to extract the coefficient of $z^k$ in the series
expansion of expression (\ref{eq:Sp}). Instead of the exact solution, we
look for an asymptotic estimate using proposition~\ref{th:ass}. For this,
we need to find the singularities of $S(z)$.

To higlight some general features of the problem, we start with a concrete
case. The left panel of Figure~\ref{fig:plotQ} shows the values of the
denominator of $S(z)$ with $p=0.1$ and $d=17$ for real $z$ around $0$. $S$
has one real root greater than $1$.  The remaining singularities of $S$
are complex and they seem to be evenly spaced on the same circle, as can
be seen on the right panel of Figure~\ref{fig:plotQ}.  This is only a
visual impression.  In fact the singularities are not exactly on the same
circle and their rotation angles are not exactly regular.

\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{singularityS.pdf}
\caption{\textbf{Singularities of $S$}. $Q(z)$ denotes the
denominator of $S(z)$ from expression (\ref{eq:Sp}) with $p=0.10$ and
$d=17$. \textit{Left}: The value of $Q(z)$ is represented for $z$ real by
the bold line. \textit{Right}: The value of $|1/Q(z)|$ is represented for
$z$ complex by the heat map on the complex plane. Darker pixels correspond
to higher values. Sixteen singularities of $S$ lie close to a circle. The
remaining seventeenth is the one shown on the left panel. It is the
dominant singularity because it is the closest to the origin.}
\label{fig:plotQ}
\end{figure}

It is fortunate that the dominant root of $S$ is a real number because we
can use efficient numerical methods to approximate it (\textit{e.g.}
bisection or the Newton-Raphson method). The following proposition shows
that this is no accident: the dominant singularity of $S$ is always a real
positive number greater than $1$.

\begin{proposition}
\label{th:roots}
$S$ as expressed in (\ref{eq:Sp}) has exactly one positive real
singularity. This is the dominant singularity and it is greater than $1$.
\end{proposition}

\begin{proof}
Write $S(z) = P(z)/Q(z)$ and search the roots of $Q$. First we show that
$Q$ has no real root in the interval $(0,1)$. For a real number $x$ such
that $0\leq x \leq 1$, we have $1+qx+\ldots+(qx)^{d-1} > (1-q^d)/p$,
so $Q(x) > 0$.

Second, we show that $Q$ has exactly one real root great than $1$. Because
$Q'(x) < 0$ for $x \geq 0$ there can be only one positive root.  Since
$Q(1) = q^d > 0$ and $\lim_{x\rightarrow \infty} Q(x) = -\infty$, $Q$
vanishes for a real number greater than $1$.

Third, we show that this is the root with smallest \textit{modulus}.
Express the complex roots of $Q$ as $Re^{i\theta}$. They satisfy the
equation

\begin{equation*}
1-pRe^{i\theta}\frac{1-q^dR^de^{id\theta}}{1-qRe^{i\theta}} = 0,
Re^{i\theta} \neq \frac{1}{q}.
\end{equation*}

Multiplying through by $1-qRe^{i\theta}$, we obtain an equation of which
we separate the real and the imaginary parts to obtain

\begin{equation*}
\left\{
\begin{array}{ll}
R \cos (\theta) -1 = pq^dR^{d+1} \cos \left( (d+1) \theta) \right) \\
R \sin (\theta) = pq^dR^{d+1} \sin \left( (d+1) \theta) \right)
\end{array}
\right. Re^{i\theta} \neq \frac{1}{q}.
\end{equation*}

Squaring and summing, we obtain the following equation

\begin{equation*}
R^2 = (pq^dR^{d+1})^2 + 2R \cos(\theta) -1.
\end{equation*}

Since $R > 0$, the solution of this equation is minimal when
$2R\cos(\theta)$ is maximal, \textit{i.e.} when $\theta = 0$. In other
words, if there is a positive real root, its \textit{modulus} is the
minimum among the roots. We have seen above that there exsists exactly one
and that it is greater than $1$.
\end{proof}

We now have all the tools to approximate the coefficients of $S(z)$ using
proposition~\ref{th:ass} and thus obtain the approximate probability that
a read contains no seed.

\begin{proposition}
\label{th:p}
The probability that a read of size $k$ is seedless is
asymptotically equivalent to

\begin{equation*}
\frac{C}{z_1^{k+1}},
\end{equation*}

\noindent
where $z_1$ is the only real positive root of the denominator of $S(z)$,
\textit{i.e.} the root of $1-pz(1+qz+\ldots+(qz)^{d-1})$, and where

\begin{equation}
\label{eq:Cp}
C =\frac{(1-qz_1)^2}{p^2 z_1 \big( 1-(d+1-dqz_1)(qz_1)^d \big)}.
\end{equation}
\end{proposition}

\begin{proof}
Apply propositions~\ref{th:ass} and \ref{th:roots}, together with the fact
that for any singularity $z$ we have $1+qz+\ldots+(qz)^{d-1} = 1/pz$.
\end{proof}

We now illustrate proposition~\ref{th:p} with a concrete example
explaining how the calculations are done in practice.

\begin{example}
\label{ex:num1}
Let us approximate the probability that a read of size $k=100$ is seedless
for $d=17$ and for a substitution rate $p=0.1$. To find the dominant
singularity of $S$, we need to solve
$1-0.1z\times(1+0.9z+\ldots+(0.9z)^{16}) = 0$. We rewrite the equation as
$1 - 0.1z\times(1-(0.9 z)^{17})/(1-0.9z) = 0$ and use bisection to solve
it numerically, yielding $z_1 \approx 1.0268856$. Substituting this value
in (\ref{eq:Cp}) yields $C \approx 1.396145$, so the probability that a
read contains no seed is approximately $1.396145 / 1.0268856^{101} \approx
0.095763$. For comparison, a 99\% confidence interval obtained by
performing 10 billion random simulations is $0.09575-0.09577$.
% The magic number is 957598614 out of 10 billion.
The computational cost of the analytic combinatorics approach is
infinitesimal compared to the random simulations, and the precision
is much higher.
\end{example}

\begin{figure}[h]
\centering
\includegraphics[scale=0.445]{simulp.pdf}
\caption{\textbf{Example estimates for substitutions only}. The analytic
combinatorics estimates of proposition~\ref{th:p} are benchmarked against
random simulations. Shown on both panels are the probablities that a read
of given size contains a seed, either estimated by 10,000,000 random
simulations (dots), or by the method described above (lines). The curves
are drawn for $d=17$ and $p=0.08$, $p=0.10$ or $p=0.12$ (from top to
bottom). The difference never exceeds 0.0003 on the examples shown here,
which is the expected variation for this number of random trials.}
\label{fig:simulp}
\end{figure}

Overall, the analytic combinatorics estimates are close to the exact
values. Figure~\ref{fig:simulp} also shows that relatively small changes
in the probability of error have a large influence on the probability of
that the read contains a seed in the depicted range of read sizes.



%%%%%%%%%%%%%%%%% Subsitutions and deletions %%%%%%%%%%%%%%%%%%

\subsection{Substitutions and deletions}
\label{sec:deletions}

The uniform substitution model does not describe all sequencing
technologies. For instance, long read technologies often have bursts of
insertions and deletions, with typical frequencies that differ from
substitutions. In order to model more complex behaviors, we need to
distinguish the different types of errors.

To not jump too fast into the difficulties, we will first focus on the
semi-realistic case where errors can be deletions or susbtitutions, but
not insertions. As in the case of uniform substitutions, we assume that
every nucletoide call is false with a probability $p$ and true with a
probability $1-p=q$. Here, we also assume the ``space''  between
consecutive nucleotides can contain a deletion with probability $\delta$.

A deletion may be adjacent to a substitution, or lie in between two
correct nucleotides. In the first case, the deletion does not interrupt
any error-free intervals so it does not change the probability that the
read contains a seed. For this reason, we ignore deletions next to
substitutions. More precisely, we assume that they can occur, but whether
they do has no importance for the problem.

A read can be thought of as a walk on the graph shown in
figure~\ref{fig:deletions}. Each nucleotide is represented as a node that
can be either a match or a substitution. The weights on the edges indicate
the probabilities of transitions between the states. As mentioned above,
deletions adjacent to substitutions are ignored; only those occurring
between matches are registered. The transition shown as a dashed line
indicates that there was a deletion between two correct nucleotides.
The other self transition of the ``Match'' state indicates that the
nucleotides are not separated by a deletion. The probabilities of these
events are $\delta q$ and $(1-\delta)q$, respectively, \textit{i.e.} the
probability that a deletion does or does not occur, times the probability
that the nucleotide is correct. The other transitions are as in the error
model with substitutions shown in
figure~\ref{fig:subonly}.

\begin{figure}[h]
\centering
\includegraphics[scale=0.9]{deletions.pdf}
\caption{\textbf{Error model with substitutions and deletions}. 
Substitution and deletion rates $p$ and $\delta$ are assumed to be
constant throughout the read. Deletions are implicit and do not have a
state of their own. Deletions before or after a substitution are ignored
because they do not affect whether the read has a seed or not.}
\label{fig:deletions}
\end{figure}

As in section~\ref{sec:substitutions}, we need to find the weighted
generating function of error-free intervals. When their size is $1$, this
is just $qz$. In an error-free interval of size $k$, there are $k-1$
``spaces'' between the nucleotides, so the weighted generating function is
$(1-\delta)^{k-1}(qz)^k$. Summing for all the possible sizes, we obtain
the weighted generating function of error-free intervals as

\begin{equation}
\label{eq:Fdel}
F(z) = qz + (1-\delta)(qz)^2 + (1-\delta)^2(qz)^3 \ldots =
\frac{qz}{1-(1-\delta)qz}.
\end{equation}

The only difference with the transfer matrix of
section~\ref{sec:substitutions} is that an error-free interval can now
follow another error-free interval if there is a deletion between them.
Since the probability of this deletion is $\delta$, the transfer matrix is


\begin{equation*}
M(z) = 
\begin{blockarray}{ccc}
       & \smDELz & \smS \\
\begin{block}{c[cc]}
\smDELz & \delta F(z) & pz \\
\smS    &        F(z) & pz \\
\end{block}
\end{blockarray}.
\end{equation*}

To apply proposition~\ref{th:TM2WGF} we need to invert $I-M(z)$, which is
straightforward for a $2 \times 2$ matrix. We obtain

\begin{equation*}
M_*(z) = (I-M(z))^{-1}=
\frac{1}{\lambda(z)}
\left(
\begin{matrix}
1-pz  & pz              \\
F(z) & 1 -\delta F(z)
\end{matrix}
\right),
\end{equation*}

\noindent
where $\lambda(z) = 1-pz-(pz(1-\delta)+\delta)F(z)$ is the determinant of
$I-M(z)$.

As in section~\ref{sec:substitutions}, the row vector $U(z)$ of starting
states is $(F(z), pz)$, and the column vector of end states is $(1,1)$, so
we obtain the weighted generating function of reads as

\begin{equation}
\label{eq:Rdel}
\begin{split}
R(z) &= 1 + U(z) \cdot M_*(z) \cdot V \\
&= \frac{1+(1-\delta)F(z)} {1-pz - \big(pz(1-\delta) + \delta\big)F(z)}
= \frac{1}{1-z}.
\end{split}
\end{equation}

The terms of equation (\ref{eq:Rdel}) cancel out when $F$ is defined as in
(\ref{eq:Fdel}). The result is $1/(1-z) = 1+z +z^2 + \ldots$, which means
that the total weight of reads of size $k$ is equal to $1$ for every $k
\geq 0$.

To find the weighted generating function of seedless reads, we need to
limit error-free intervals to a maximum size of $d-1$, \textit{i.e.} to
replace $F(z)$ by its truncation $F_d(z) = qz + (1-\delta)(qz)^2 + \ldots
+ (1-\delta)^{d-2}(qz)^{d-1}$. With this definition, the weighted
generating function of seedless reads is

\begin{equation}
\label{eq:Sdel}
S(z) = \frac{1+(1-\delta)F_d(z)}
  {1-pz - \big(pz(1-\delta) + \delta\big)F_d(z)}.
\end{equation}

Applying proposition~\ref{th:ass} to this expression, we obtain the
following proposition.

\begin{proposition}
\label{th:pd}
The probability that a read of size $k$ is seedless is asymptotically
equivalent to

\begin{equation*}
\frac{C}{z_1^{k+1}},
\end{equation*}

\noindent
where $z_1$ is the only real positive root of the denominator of $S(z)$,
\textit{i.e.} the root of $1-pz - \big(pz(1-\delta) +
\delta\big)\big(qz+(1-\delta)(qz)^2 + \ldots +
(1-\delta)^{d-2}(qz)^{d-1}\big)$, and

\begin{equation}
\label{eq:Cpd}
\begin{split}
C &=
\frac{ z_1 \big(1-(1-\delta)(1-p)z_1\big)^2 }
{ \big((p+q\delta)z_1  -\gamma^*(1-\delta)^{d-1}(qz_1)^d \big)
\big(\delta+(1-\delta)pz_1\big) }, \\
\gamma^* &= d\delta -(1-\delta)\big((d-1)\delta-p((d-1)\delta+d+1)\big)z_1
- d(1-\delta)^2pqz_1^2.
\end{split}
\end{equation}
\end{proposition}

\begin{remark}
Notice that the power of $z_1$ differs between propositions~\ref{th:p} and
\ref{th:pd}. The reason is that the factors $z$ or $1/z$ have been moved
out of the constant $C$ in expressions (\ref{eq:Cp}) and (\ref{eq:Cpd}).
\end{remark}

\begin{example}
\label{ex:num2}
Let us approximate the probablity that a read of size $k = 100$ is
seedless for $d=17$, $p = 0.05$ and $\delta = 0.15$. In order to find the
dominant singularity of $S$ expressed as in (\ref{eq:Sdel}), we need to
solve the equation $1-0.05z - \big(0.0425z + 0.15\big)
\big(0.95z+0.85(0.95z)^2 + \ldots + 0.85^{15}(0.95z)^{16}\big) = 0$. We
write it as $1-0.05z - \big(0.0425z + 0.15) (0.95z-0.85^{16}(0.95z)^{17})
/ (1-0.8075z) = 0$ and use bisection to solve it, yielding $z_1 \approx
1.006705$. Now substituting the obtained value in (\ref{eq:Cpd}) gives $C
\approx 1.096177$, so the probability that a read contains no seed is
approximately $1.096177 / 1.006705^{101} \approx 0.558141$. For
comparison, a 99\% confidence interval obtained by performing 10 billion
random simulations is $0.55813-0.55816$.
% The magic number is 5581417733 out of 10 billion.
\end{example}

\begin{figure}[h]
\centering
\includegraphics[scale=0.445]{simulpdel.pdf}
\caption{\textbf{Example estimates for substitutions and deletions}. The
analytic combinatorics estimates of proposition~\ref{th:pd} are
benchmarked against random simulations. Shown on both panels are the
probablities that a read of given size contains a seed, either estimated
by 10,000,000 random simulations (dots), or by the method described above
(lines). The curves are drawn for $d=17$, $p=0.05$ and $\delta=0.14$,
$\delta=0.15$ or $\delta=0.16$ (from top to bottom). The difference never
exceeds 0.0003 on the examples shown here, which is the expected variation
for this number of random trials.}
\label{fig:simulpdel}
\end{figure}




%%%%%%%%%%% Subsitutions, deletions and insertions %%%%%%%%%%%%

\subsection{Substitutions, deletions and insertions}
\label{sec:insertions}

Introducing insertions brings two additional difficulties. The first is
that substitution is indistinguishable from an insertion followed by a
deletion (or a deletion followed by an insertion). By convention, we will
count all these cases as substitutions. This entails that a deletion can
never be found next to an insertion. The second difficulty is that
insertions usually come in bursts. This is also the case of deletions, but
we could neglect it because this does not affect the size of the interval
(all deletions have size $0$). 

We still denote the probability of substitution as $p$ and the probability
of deletion $\delta$. To model insertion bursts, we need to assign a
probability $r$ to the first insertion, and a probability $\tilde{r} > r$
to all subsequent insertions of the burst. We will still denote the
probability of a correct call as $q$, but here $q = 1-p-r$.  An insertion
burst stops with probability $1-\tilde{r}$ at each position, sandt is
followed by either a match or a substitution with probability $1-p/(1-r) =
q/(1-r)$ and $p/(1-r)$, respectively. Figure~\ref{fig:insertions} shows
the transition probabilities between matches, substitutions and
insertions.

\begin{figure}[h]
\centering
\includegraphics[scale=0.9]{insertions.pdf}
\caption{\textbf{Error model with substitutions, deletions and
insertions}. Rates of substitution ($p$), deletion ($\delta$) and
insertion ($r$) are assumed to be contant throughout the read. The
prolongation rate of insertions ($\tilde{r}$) is also assumed to be
constant. Here $q = 1-p-r$ and the transitions from ``Inertion'' to
``Match'' and ``Substitution'' are normalized to maintain a constant rate
of substitution.}
\label{fig:insertions}
\end{figure}

As in section~\ref{sec:deletions}, we must ensure that there is no deletion
between any two positions of an error-free interval. The weighted
generating function thus comes as

\begin{equation*}
F(z) = qz + (1-\delta)(qz)^2 + (1-\delta)^2(qz)^3 \ldots =
\frac{qz}{1-(1-\delta)qz}.
\end{equation*}

This expression is the same as in the case of section~\ref{sec:deletions},
but here $q = 1-p-r$ instead of $1-p$. Based on the error model and the
conventions, the transfer matrix for the states ``match'',
``substitution'' and ``insertion'' is

\begin{equation*}
M(z) = 
\begin{blockarray}{cccc}
       & \smDELz & \smS & \smI \\
\begin{block}{c[ccc]}
\smDELz & \delta F(z) & pz & rz \\
\smS    &        F(z) & pz & rz \\
\smI    & \frac{1-\tilde{r}}{1-r}F(z)
           & \frac{1-\tilde{r}}{1-r}pz & \tilde{r}z \\
\end{block}
\end{blockarray}.
\end{equation*}


The expression of $M_*(z) = (I-M(z))^{-1}$ is omitted because it is
cumbersome and all we need is the value if $1+U(z)\cdot M_*(z)\cdot V$.
We still give the determinant of $I-M(z)$ in full because this will be the
denomiator of the final expression. Up to a factor $1-r$, it is equal to
$1-a(z)-b(z)F(z)$, where

\begin{gather}
\label{eq:a+b}
a(z) = r-\big((p+\tilde{r})r-\tilde{r}-p\big)z
- p(\tilde{r}-r)z^2\text{, and} \\
b(z) = \delta(1-r) - \big((\tilde{r}\delta-(1-\delta)p)(1-r)
-(1-\tilde{r})r\big)z -(1-\delta)p(\tilde{r}-r)z^2.
\notag
\end{gather}

Applying proposition~\ref{th:TM2WGF} the weighted generating function of
reads appears as

\begin{equation}
\label{eq:Rindel}
\begin{split}
R(z) &= 1 + U(z) \cdot M_*(z) \cdot V \\
&= \frac{(1-r)\big( 1-(\tilde{r}-r)z \big)
\left(1+(1-\delta)F(z) \right)}{1-a(z)-b(z)F(z)}
= \frac{1}{1-z}.
\end{split}
\end{equation}

Again, the terms cancel out and we obtain the simple expression
$1/(1-z)$ where all the coefficients are equal to $1$. To find the
weighted generating function of seedless reads, we replace $F(z)$ in
expression (\ref{eq:Rindel}) by its truncated version

\begin{equation*}
\begin{split}
F_d(z) &= qz + (1-\delta)(qz)^2 + (1-\delta)^2(qz)^3 \ldots +
(1-\delta)^{d-2}(qz)^{d-1} \\
&= \frac{qz-(1-\delta)^{d-1}(qz)^d}{1-(1-\delta)qz}.
\end{split}
\end{equation*}

We obtain the following expression, where $a(z)$ and $b(z)$ are defined in
(\ref{eq:a+b})

\begin{equation}
\label{eq:Sindel}
S(z) = \frac{(1-r)\big( 1-(\tilde{r}-r)z \big) \left(1+(1-\delta)F_d(z)
\right)}{1-a(z)-b(z)F_d(z)}.
\end{equation}


\begin{proposition}
\label{th:pins}
The probability that a read of size $k$ is seedless is asymptotically
equivalent to

\begin{equation*}
\frac{C}{z_1^{k+1}},
\end{equation*}

\noindent
where $z_1$ is the only real positive root of the polynomial
$1-a(z)-b(z)F_d(z) = 1-a(z)-b(z)\big(qz+(1-\delta)(qz)^2 + \ldots +
(1-\delta)^{d-2}(qz)^{d-1}\big)$, and $C = \alpha / \beta$, with

\begin{equation*}
\alpha = (1-r)(1-(\tilde{r}-r)z_1)
   \frac{1-\big((1-\delta)(qz)\big)^d}{1-(1-\delta)qz}
\end{equation*}

\noindent
and
\begin{equation*}
\begin{split}
\beta = a'(z_1) &- \left( b'(z_1) +
\frac{(1-\delta)q}{1-(1-\delta)qz}\right)
\frac{qz-(1-\delta)^{d-1}(qz)^d}{1-(1-\delta)qz} \\
&+b(z_1) \frac{q-d(1-\delta)^{d-1}q^dz^{d-1}}{1-(1-\delta)qz}.
\end{split}
\end{equation*}
\end{proposition}

Note that the constant $C$ above has an explicit expression for a given
value of $z_1$, but it is relatively cumbersome, so it is more useful to
show how to compute it. We now show how to apply propostion~\ref{th:pins}
in a practical case.

\begin{example}
\label{ex:num3}
Let us approximate the probability that a read of size $k=100$ is seedless
for $d=17$ for $p=0.05$, $\delta=0.15$, $r=0.05$ and $\tilde{r}=0.45$.
With these values, $a(z) = 0.05 +0.475z -0.02z^2$ and $b(z) = 0.1425 +
0.00375z-0.017z^2$. We need to solve $0.95-0.475z+0.02z^2 - (0.1425
+0.00375z-0.017z^2)(0.9z+0.85(0.9z)^2+\ldots+0.85^{15}(0.9z)^{16}) = 0$.
We rewrite the equation as $0.95-0.475z+0.02z^2 - (0.1425
+0.00375z-0.017z^2)(0.9z-0.85^{15}(0.9z)^{16})/(1-0.765z) = 0$ and use
bisection to solve it numerically, yielding $z_1 \approx 1.00295617$.
Using proposition~\ref{th:pins}, we obtain $C \approx 1.042504$, so the
probability that a read contains no seed is approximately $1.042504 /
1.00295617^{101} \approx 0.773749$. For comparison, a 99\% confidence
interval obtained by performing 10 billion random simulations is
$0.77373-0.77376$.
% The magic number is 7737489777 out of 10 billion.
\end{example}

\begin{figure}[h]
\centering
\includegraphics[scale=0.445]{simulpins.pdf}
\caption{\textbf{Example estimates for substitutions, deletions and
insertions}. The analytic combinatorics estimates of
proposition~\ref{th:pins} are benchmarked against random simulations.
Shown on both panels are the probablities that a read of given size
contains a seed, either estimated by 10,000,000 random simulations (dots),
or by the method described above (lines). The curves are drawn for $d=17$,
$p=0.05$, $\delta=0.15$, $\tilde{r} = 0.45$ and $r=0.04$, $r=0.05$ or
$r=0.06$ (from top to bottom). The difference never exceeds 0.0002 on the
examples shown here, which is the expected variation for this number of
random trials.}
\label{fig:simulpins}
\end{figure}


\begin{remark}
Note that when $r = \tilde{r} = 0$, $a(z) = pz$ and $b(z) = pz(1-\delta) +
\delta$, so expression (\ref{eq:Sindel}) becomes

\begin{equation*}
S(z) = \frac{1 + (1-\delta)F_d(z)}{1-pz-(pz(1-\delta)+\delta))F_d(z)}.
\end{equation*}

This is expression (\ref{eq:Sdel}), \textit{i.e.} the model described in
section~\ref{sec:deletions}. When we also have $\delta = 0$, this
expression further simplifies to

\begin{equation*}
S(z) = \frac{1 + F_d(z)}{1-pz(1 + F_d(z))}.
\end{equation*}

This is expression (\ref{eq:Sp}), \textit{i.e.} the model described in
section~\ref{sec:substitutions}. In other words, the error models
described previously are special cases of this one.
\end{remark}



%%%%%%%%%%%%%%%%%%%% Empirical error models %%%%%%%%%%%%%%%%%%

\subsection{Empirical error models}
\label{subsec:empirical}

In the theory developed here, the only variable of interest is the
probability distribution of the error intervals. Indeed, the only reason
we introduced different kinds of errors is because they have different
probabilities and different sizes, but the nature of the error is
irrelevant.

In addition to error-free intervals, we introduce error-only intervals,
which will encapsulate the available information about the size of error
patches. As we will see below, we need to keep error-free intervals and
deletions separate, so error-only intervals are always considered to be
non-empty.

\begin{definition}
\label{def:error-interval}
An \textbf{error-only interval} is a non-empty sequence of errors that
cannot be extended left or right.
\end{definition}

If we have empirical probabilities $p_0, p_1, \ldots, p_n$ such that $p_k$
is the probability that the error pach has size $k$, we can directly write
the weighted generating function of non-empty error intervals as $E(z) =
p_1z + p_2z^2 + \ldots + p_nz^n$. If the terms are estimated by averaging
observations, this function is a polynomial and $n$ is the size of largest
observed error patch.

The weighted generating function of deletions is simply $p_0$. We will set
$\delta = p_0$ for consistency with the previous sections. We need to keep
deletions separate from error-only intervals because a read cannot start
or end with a deletion (there is no way to detect them before the read
starts or after it finishes). For this reason, we must keep one extra
state of the transfer matrix for deletions. But we must remember that an
uninterrupted error patch was either a deletion or an error-only interval,
so deletions and error-free intervals cannot lie next to each other.

An error-free interval can be followed by an error-only interval or by a
deletion, but it cannot be followed by another error-free interval. An
error-only interval can only be followed by an error-free interval.
Similarly, a deletion can only be followed by an error-free inerval. The
transfer matrix of the objects ``error-free interval'', ``error-only
interval'' and ``deletion'' is


\begin{equation*}
M(z) = 
\begin{blockarray}{ccc}
       & \smDELz & \smDELs \\
\begin{block}{c[cc]}
\smDELz & \delta F(z)  & E(z) \\
\smDELs & F(z)         & 0    \\
\end{block}
\end{blockarray}.
\end{equation*}

Since the sequence may neither start nor end with a deletion, the row
vector of start objects $U(z)$ is equal to $(F(z), E(z), 0)$ and the
column vector $V$ of end objects is equal to $(1,1,0)$. The weighted
generating function of reads can be expressed in general terms as

\begin{equation}
\label{eq:Remp_gen}
R(z) = 1 + U(z) \cdot (I-M(z))^{-1} \cdot V =
\frac{\big(1+(1-\delta)F(z)\big)\big(1+E(z)\big)}
   {1-F(z)\big(\delta+E(z)\big)}.
\end{equation}

As previously, the weighted generating function of error-free intervals is
$F(z) = qz + (1-\delta)(qz)^2 + (1-\delta)^2(qz)^3 + \ldots =
qz/(1-(1-\delta)qz)$. Substituting this value in the equation above, we
obtain

\begin{equation}
\label{eq:Remp}
R(z) = \frac{1+E(z)}{1-qz\big(1+E(z)\big)}.
\end{equation}

Epxression (\ref{eq:Remp}) is not equal to $1/(1-z)$ because $E(z)$ is in
general not equal to $pz/(1-pz)$. This means that the total weight of
reads of size $k$ is not equal to $1$, so we will need to take this
into account in our approximations. As usual, we will use
proposition~\ref{th:ass}, implying that we have to find the root with
smallest \textit{modulus} of the polynomial $1-qz\big(1+E(z)\big)$.

To find the weighted generating function of seedless reads, we need to
replace $F(z)$ by its truncation $F_d(z) = qz + (1-\delta)(qz)^2 + \ldots
+ (1-\delta)^{d-2}(qz)^{d-1}$ in expression (\ref{eq:Remp_gen}). The terms
only partially cancel each other and we obtain

\begin{equation}
\label{eq:Semp}
S(z) = \frac{\big(1+E(z)\big)\big( 1-(1-\delta)^d(qz)^d \big)}
{1-(1-\delta)qz-qz\big(1-(1-\delta)^{d-1}(qz)^{d-1}\big)
\big(\delta+E(z)\big) }.
\end{equation}

To estimate the total weight of seedless reads, we need to find the root
with smallest \textit{modulus} of $1-qz\big(1+E(z)\big) +
(1-\delta)^{d-1}(qz)^{d-1}\big(\delta+E(z)\big)$. The solution depends on
the particular expression of $E(z)$. Even though the process can be
automated using proposition~\ref{th:ass}, every case is different and
we cannot give an explicit formula here.

\begin{example}
Assume that a series of empirical measurements suggest that $q = 0.9$,
$\delta = 0.1$ and that error patches have size $1$, $2$ or $3$ have
probabilities $0.5$, $0.33$ and $0.17$. This implies that $E(z) = 0.5z +
0.33z^2+0.17z^3$.

If we choose seeds of size $d=17$, the weighted generating function of
reads show in (\ref{eq:Remp}) becomes

\begin{equation*}
R(z) = \frac{1+0.5z +0.33z^2+0.17z^3}
{1-0.9z\big(1+0.5z +0.33z^2+0.17z^3\big)}.
\end{equation*}

The smallest root of the denominator is approximately equal to $0.1946949$
and applying proposition~\ref{th:ass}, the multiplicative constant is
approximately equal to $1.905695$, so the total weight of reads of size
$k$ is approximately equal to $1.905695/0.1946949^{k+1}$.

Similary, the weighted generating function of seedless reads shown in
(\ref{eq:Semp}) becomes

\begin{equation*}
S(z) = \frac{\big(1+0.5z +0.33z^2+0.17z^3\big)\big( 1-(0.81z)^{17} \big)}
{1-0.81z-0.9z(1-(0.81z)^{16})(0.1+0.5z +0.33z^2+0.17z^3)}.
\end{equation*}

The smallest root of the denominator is approximately $0.0122100$ and the
ultiplicative constant is approximately equal to $0.0135667$. So the total
weight of seedless reads of size $k$ is approximately equal to $0.0135667
/ 0.0122100^{k+1}$.

Combining these two results, we obtain the probability that a read of size
$k$ has no seed as the ratio of the total weights computed above. This is
approximately equal to $0.007119 / 0.06271356^{k+1}$.
\end{example}



\section{Caveats}

\subsection{Short reads with low error rates}
The main motivation for the analytic combinatorics approach developed
above is to find estimates that converge exponentially fast to the target
value. Propositions~\ref{th:ass} and \ref{th:avsub} show that the rate of
convergence is dominated by the ratio between the dominant singularity and
the second largest singularity. This means that convergence slows down as
the dominant singularity 

The worst case for the approximation is thus when the reads are
small and when the first two singularities have close \textit{moduli}.

All the examples chosen so far showed that the analytic combinatorics
estimates are accurate.

In practice, these are the conditions of the Illumina technology, where
errors are almost always substitutions, occurring at a frequency around
1\% on current instruments. This corresponds to the error model of
section~\ref{sec:substitutions} with substitutions only, where the
probability of substitution $p$ is approximately equal to $0.01$. As $p$
approaches $0$, the \textit{modulus} of the dominant singularity get
closer to the others, so the rate of convergence slows down.

Since the reads are often around 50 nucleotides, the analytic
combinatorics estimates of the seeding probabilities are typically less
accurate than suggested in the previous sections. The example below shows
the accuracy of the estimates in one of the worst cases.


\begin{figure}[h]
\centering
\includegraphics[scale=0.445]{simulp_short.pdf}
\caption{\textbf{Example worst case estimates for seeding with
substitutions only}. The analytic combinatorics estimates of
proposition~\ref{th:p} are benchmarked against 10,000,000 random
simulations. Shown on both panels are the probablities that a read of
given size contains a seed of size $d=17$, either estimated by random
simulations (dots), or by the estimate derived in
section~\ref{sec:substitutions}. The curves are drawn for $p=0.005$,
$p=0.010$ or $p=0.015$ (from top to bottom). The largest difference
between the estimates and the simulations is around $0.01$.}
\label{fig:simulp_short}
\end{figure}





\subsection{Simplified error models}

For the purpose of computing the asymptotic seeding probabilities, the
substitutions error model of section~\ref{sec:substitutions} is
substantially simpler than the models with deletions and insertions
described in sections~\ref{sec:deletions} and \ref{sec:insertions}.

When dealing with insertions and deletions, it is thus tempting to
approximate the error model by a substitution model with equivalent error
rate. A natural choice is to set the parameter $q$ of the first model to
the value of $q(1-\delta)$ because they both represent the probability of
decoding a nucleotide without error.

However, this approach is substantially less accurate. To show this, let
us revisit example~\ref{ex:num3} with an approximate substitution model
instead of the full error model with substitutions, insertions and
deletions.

\begin{example}
\label{ex:num4}
In example~\ref{ex:num3}, we computed the approximate probability that a
read of size $k=100$ contains no $17$-seed, where the probability of
substitution $p=0.05$, the probability of deletion $\delta=0.15$, the
probability of insertion $r=0.05$ and the probability of continued
insertion $\tilde{r}=0.45$. In such conditions, the probability of
decoding a nucleotide without error is $(1-p-r)(1-\delta) = 0.765$.
Now using a substitution model where $q = 0.765$ and thus $p = 0.235$, the
analytic combinatorics estimate comes out as $1.03726/1.002591^{101}
\approx 0.79870$. This number is outside the 99\% confidence
$0.77373-0.77376$ by about $2.5$ percentage points.
\end{example}

The approximation error does not vanish. Actually, in the conditions of
example~\ref{ex:num4} it increases with the read size $k$.
Figure~\ref{fig:simulp_approx} shows the same data as
figure~\ref{fig:simulpins}, now fitted by an approximate substitution
error. The error can be as high as $5$ percentage points.


\begin{figure}[h]
\centering
\includegraphics[scale=0.445]{simulp_approx.pdf}
\caption{\textbf{Title} Caption.} 
\label{fig:simulp_approx}
\end{figure}

The conclusion is that the quality of the approximation collapses when
using approximate error models. One may argue that an error of 5
percentage points may be tolerable, but the actual amount of error when
using an approximate error model is unknown in genereal. Unsing simplified
error models is unsafe because convergence is slow and the error cannot be
controlled.

Nevertheless, complexity comes at a cost. It is important to remember that
the parameters of the error model may be inaccurate, in which case the
error can also not be controlled. If these parameters are hard to
estimate, it may be preferrable to use a simplified model. But in general,
the amount of data available in a sequencing run is sufficient to estimate
the parameters of the error model. In practice, the safest approach to
compute seeding probabilities is to use the full error model described in
section~\ref{sec:insertions} and estimate the four parameters of the full
error model from the alignment data generated during the mapping.








%%%%%%%%%%%%%%%%%% Inexact seeding %%%%%%%%%%%%%%%%%%%%%
\section{Inexact seeding}

We now consider a more challenging problem. The ongoing development of
algorithms and data structures makes it possible to search inexact seeds,
\textit{i.e.} sequences that are very similar but not identical to the
target. This comes at greater cost than finding exactly similar sequences,
but it may be worth it because the chances are higher to identify the
target. By tolerating errors in the seed, one can look for longer seeds,
which also reduces the false positive rate.

Before showing how to compute the probabilities that a read contains
an inexact seed, let us roll back to the simplest error model that we have
see so far, namely substitutions only. Figure~\ref{fig:sketchinexact}
illustrates graphically the relationship between intervals, substitutions
and seeds.

\begin{figure}[h]
\centering
\includegraphics[scale=0.88]{sketch_inexact_seeding.pdf}
\caption{\textbf{Inexact seeding}. Substitutions (black squares) occur
uniformly at random within the read. They delimit error-free intervals.
Inexact seeds (arrows) are the longest stretches of the read that contain
at most one sbustituttion.}
\label{fig:sketchinexact}
\end{figure}


\begin{definition}
\label{def:seed}
An \textbf{inexact $d$-seed} is an interval of size $d$ or greater that
contains at most a substitution and no other error.
\end{definition}

In other words, an inexact seed is either an error-free interval of size
$d$ or greater, or the concatenation of two error-free intervals separated
by a substitution, with total size $d$ or greater. An exact $d$-seed is an
inexact $d$-seed, but the converse is not true. In this section, a
``seedless'' read will designate a read that does not contain any inexact
seed.

\begin{figure}[h]
\centering
\includegraphics[scale=0.88]{inexact_graph.pdf}
\caption{\textbf{Transfer in inexact seeding}.
Legend goes here.}
\label{fig:simulp_fp}
\end{figure}

As in the case of exact seeding, we will give a construction of seedless
reads, find the corresponding weighted generating function and extract the
asymptotic behavior of the coefficients. Transfer matrices will come in
handy to express the dependence between the error-free intervals on either
side of a substitution. Because the read is seedless, the sum of the sizes
of the intervals on the left hand side and on the right hand side must not
exceed $d-2$ (the substitution adds $1$ to the size of the seed).

In other words, an error-free interval of size $k$ can be followed by an
error-free interval of size $0$, $1$, $2$, \ldots, or $d-k-2$. These
intervals have respective weighted generating function $1$, $qz$,
$(qz)^2$, \ldots, $(qz)^{d-k-2}$, so the transfer matrix is

\begin{equation*}
M(z) = 
\begin{blockarray}{ccccccc}
       & \scriptstyle{0} & \scriptstyle{1} & \scriptstyle{2} &
    \ldots &  \scriptstyle{d-2} & \scriptstyle{d-1} \\
\begin{block}{c[cccccc]}
\scriptstyle{0} & pz  & (qz)pz & (qz)^2pz & \ldots &
    (qz)^{d-3}pz & (qz)^{d-2}pz \\
\scriptstyle{1} & pz  & (qz)pz & (qz)^2pz & \ldots &
    (qz)^{d-3}pz & 0 \\
\scriptstyle{2} & pz  & (qz)pz & (qz)^2pz & \ldots &
    0 & 0 \\
\vdots & \vdots  & \vdots & \vdots & \ddots & \vdots & \vdots  \\
\scriptstyle{d-3} & pz  & (qz)pz & (qz)^2pz & \ldots & 0 & 0 \\
\scriptstyle{d-2} & pz  & (qz)pz & 0 & \ldots & 0 & 0 \\
\scriptstyle{d-1} & pz  & 0      & 0 & \ldots & 0 & 0 \\
\end{block}
\end{blockarray}.
\end{equation*}


Note that all the terms are multiplied by $pz$ because we have to append
a substitutoin, with weighted generating function $pz$.

Since the sequence may start with an error-only interval of any size from
$0$ to $d-2$, the vector of starting ojects $U(z)$ is $(1, qz, (qz)^2,
\ldots, (qz)^{d-2})$, and because the same holds for the end of the
squence, the vector of end state $V$ is $(1,1, \ldots, 1)^T$.

By proposition~\ref{th:TM2WGF}, the weighted generating function of
seedless reads can be computed as $1+U(z) \cdot (I-M(z))^{-1} \cdot V$.
This expression is not easy to evaluate for large matrices, but it can be
pre-computed for a useful range of values of $d$. The solution can always
be expressed as the ratio of two polynomials, and even though their degree
is high, the asymptotics can be found easily by numerical approaches.

\begin{example}
Let us revisit example~\ref{ex:num1}, where we approximated the
probability that a read of size $k=100$ is seedless for $d=17$ and for a
substitution rate $p=0.1$; but this time we allow the seed to contain one
substitution.  The matrix $M(z)$ is $16\times16$ and the weighted
generating function $1+U(z)\cdot(I-M(z))^{-1}\cdot V$ can be written as
$P(z)/Q(z)$, where $P$ is a polynomial of degree 152 and $Q$ is a
polynomial of degree 153.  Using numerical methods to find the root of $Q$
with smallest \textit{modulus}, we obtain $z_1 \approx 1.079244$.
Likewise, we obtain $-P(z_1)/Q'(z_1) \approx 2.326700$, so the probability
that the read is seedless is approximately $2.326700/1.079244^{101}
\approx 0.0010511$.  For comparison, a 99\% confidence interval obtained
by performing 10 billion random simulations is $0.001049-0.001054$.
% The magic number is 1051660 out of 10 billion.
In these conditions, the chances that the read contains an exact seed is
90.4\%, whereas the chances that it contains a seed with one substitution
is 99.9\%.
\end{example}


\begin{figure}[h]
\centering
\includegraphics[scale=0.445]{simulp-inexact.pdf}
\label{fig:simulpinexact}
\caption{\textbf{Example estimates for inexact seeding with substitutions
only}. The analytic combinatorics estimates of proposition~\ref{th:pins}
are benchmarked against random simulations.  Shown on both panels are the
probablities that a read of given size contains an inexact seed, either
estimated by random simulations (dots), or by the method described above
(lines). The curves are drawn for $d=17$, $p=0.08$, $p=0.10$ or $p=0.12$
(from top to bottom). 10,000,000 simulations are run on the left panel and
100,000,000 on the right one.  The difference never exceeds 0.0002 on the
examples shown here, which is the expected variation for this number of
random trials.}
\end{figure}






%%%%%%%%%%%%%%%%%% False positives %%%%%%%%%%%%%%%%%%%%%
\section{False positives}

As a heuristic approach, seeding can make two types of errors: false
positives and false negatives. So far we have been concerned with the
probability of false negatives because useful estimates were readily
available from the standard analytic combinatorics theory. Such estimates
are significantly more difficult to obtain for false positives. The main
reason is that the results largely depend on the repeat structure of the
genome.

Genomes are not a succession of random nucleotides. Large sequences are
often duplicated, to the extent that the majority of the genome can...


\begin{figure}[h]
\centering
\includegraphics[scale=0.88]{sketch_dual_mutations.pdf}
\caption{\textbf{False seeding}. Some description.}
\label{fig:sketchdual}
\end{figure}

Each decoded nucleotide can match 1. both sequences, 2. only the target
sequence, 3. match only the duplicate sequence or 4. match none of them.
The probabilities of these events are denoted $a$, $b$, $c$ and $d$, and
we obviously have $a+b+c+d=1$.
A nucleotide is a mismatch for both sequences if it is a read error
(probability $q$) and if either the decoy is identical to the target
(probability $1-\varepsilon$) or is different from both the target and the
decoded nucleotide (probability $2\varepsilon/3$).

\begin{equation*}
\begin{blockarray}{cccccccc}
   & \scriptstyle{0,0} & \scriptstyle{0,1} & 
    \ldots & \scriptstyle{0,d-1} &
    \scriptstyle{1,0} & \ldots &
    \scriptstyle{d-1,0}\\
\begin{block}{c[ccccccc]}
\scriptstyle{0,0} & dz\frac{1-(az)^d}{1-az}  & cz & \ldots &
    a^{d-2}cz^{d-1} & bz & \ldots & a^{d-2}bz^{d-1} \\
\scriptstyle{0,1} & dz\frac{1-(az)^{d-1}}{1-az} \\
\scriptstyle{0,2} & dz\frac{1-(az)^{d-2}}{1-az} \\
\vdots & \vdots & & A(z) & & & B(z) \\
\scriptstyle{0,d-1} & dz \\
\scriptstyle{1,0} & dz\frac{1-(az)^{d-1}}{1-az} \\
\scriptstyle{2,0} & dz\frac{1-(az)^{d-2}}{1-az} \\
\vdots & \vdots & & C(z) & & & D(z) \\
\scriptstyle{d-1,0} & dz \\
\end{block}
\end{blockarray},
\end{equation*}

\noindent
where $A(z)$, $B(z)$, $C(z)$ and $D(z)$ are square matrices defined as

\begin{equation*}
A(z) = 
\begin{blockarray}{cccccc}
   & \scriptstyle{0,1} & \scriptstyle{0,2} & \ldots &
    \scriptstyle{0,d-2} & \scriptstyle{0,d-1} \\
\begin{block}{c[ccccc]}
\scriptstyle{0,1} & 0 & cz & \ldots &
    a^{d-3}cz^{d-2} & a^{d-2}cz^{d-1} \\
\scriptstyle{0,2} & 0 & 0 & \ldots &
    a^{d-4}cz^{d-3} & a^{d-3}cz^{d-2} \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
\scriptstyle{0,d-2} & 0 & 0 & \ldots & 0 & cz \\
\scriptstyle{0,d-1} & 0 & 0 & \ldots & 0 & 0 \\
\end{block}
\end{blockarray},
\end{equation*}

\begin{equation*}
B(z) = 
\begin{blockarray}{cccccc}
   & \scriptstyle{1,0} & \scriptstyle{2,0} & \ldots &
    \scriptstyle{d-2,0} & \scriptstyle{d-1,0} \\
\begin{block}{c[ccccc]}
\scriptstyle{0,1} & bz & abz^2 & \ldots &
    a^{d-3}bz^{d-2} & a^{d-2}bz^{d-1} \\
\scriptstyle{0,2} & bz & abz^2 & \ldots & a^{d-3}bz^{d-2} & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
\scriptstyle{0,d-2} & bz & abz^2 & \ldots & 0 & 0 \\
\scriptstyle{0,d-1} & bz & 0 & \ldots & 0 & 0 \\
\end{block}
\end{blockarray},
\end{equation*}

\begin{equation*}
C(z) = 
\begin{blockarray}{cccccc}
   & \scriptstyle{0,1} & \scriptstyle{0,2} & \ldots &
    \scriptstyle{0,d-2} & \scriptstyle{0,d-1} \\
\begin{block}{c[ccccc]}
\scriptstyle{1,0} & cz & acz^2 & \ldots &
    a^{d-3}cz^{d-2} & a^{d-2}cz^{d-1} \\
\scriptstyle{2,0} & cz & acz^2 & \ldots & a^{d-3}cz^{d-2} & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
\scriptstyle{d-2,0} & cz & acz^2 & \ldots & 0 & 0 \\
\scriptstyle{d-1,0} & cz & 0 & \ldots & 0 & 0 \\
\end{block}
\end{blockarray},
\end{equation*}

\begin{equation*}
D(z) = 
\begin{blockarray}{cccccc}
   & \scriptstyle{1,0} & \scriptstyle{2,0} & \ldots &
    \scriptstyle{d-2,0} & \scriptstyle{d-1,0} \\
\begin{block}{c[ccccc]}
\scriptstyle{1,0} & 0 & bz & \ldots &
    a^{d-3}bz^{d-2} & a^{d-2}bz^{d-1} \\
\scriptstyle{2,0} & 0 & 0 & \ldots &
    a^{d-4}bz^{d-3} & a^{d-3}bz^{d-2} \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
\scriptstyle{d-2,0} & 0 & 0 & \ldots & 0 & bz \\
\scriptstyle{d-1,0} & 0 & 0 & \ldots & 0 & 0 \\
\end{block}
\end{blockarray},
\end{equation*}

As above, the weighted generating function of reads with no hit is given
by\footnote{This time the empty sequence in includedin the matrix
multiplication so there is no need to add $1$.} $H \cdot (I-M(z))^{-1}
\cdot T$. In this case the row vector $H$ is equal to
$(1,0,0,\ldots,0)$ and the column vector $T$ is equal to
$(1+az+\ldots+(az)^{d-1}, \ldots, 1+az, 1)$.


\begin{figure}[h]
\centering
\includegraphics[scale=0.445]{simulp_false_positives.pdf}
\caption{\textbf{Example estimates of false positive rates}. The analytic
combinatorics estimates are benchmarked against random simulations. Shown
on both panels are the probablities that a read of given size contains a
seed, either estimated by 10,000,000 (left) or 100,000,000 (right) random
simulations (dots), or by the method described above (lines). The curves
are drawn for $d=17$ and $\kappa=0.05$, $p=0.15$ or $p=0.25$ (from top to
bottom).}
\label{fig:simulp_fp}
\end{figure}


The result is a rational function
$P(z)/Q(z)$ where $P$ and $Q$ are polynomials with too many terms to fit
in this document. For instance, for seeds of size $d=17$, the degrees of
$P$ and $Q$ are 576 and 578, respectively.

All that is required in order to compute the asymptotic coefficients is
the dominant singularity $z_1$, together with the multiplicative constant
$C$ of proposition~\ref{th:ass}. Since the weighted generating function is
fully determined by the minimum seed size $d$, the error rate $p$, the
divergence rate  $\kappa$, we can pre-compute $z_1$ and $C$ for a useful
range of $d$, $p$ and $\kappa$.



\subsection{MEM seeds}

This is the transfer matrix for the reads that contain no MEM seed of size
$d$ or greater, given that the target sequence has exactly one duplicate.

\begin{equation*}
M(z) =
\begin{blockarray}{ccccccc}
   & \scriptstyle{0,0} & \scriptstyle{0,1} & \scriptstyle{0,2}
   & \ldots & \scriptstyle{0,d-1} & \scriptstyle{*,0} \\
\begin{block}{c[cccccc]}
\scriptstyle{0,0} & dz\frac{1-(az)^d}{1-az} &
    cz & acz^2 & \ldots & a^{d-2}cz^{d-1} & bz\frac{1}{1-az} \\
\scriptstyle{0,1} & dz \frac{1-(az)^{d-1}}{1-az} & 0 & cz &
    \ldots & a^{d-3}cz^{d-2} & bz\frac{1-(az)^{d-1}}{1-az} \\
\scriptstyle{0,2} & dz\frac{1-(az)^{d-2}}{1-az} & 0 & 0 &
    \ldots & a^{d-4}cz^{d-3} & bz\frac{1-(az)^{d-2}}{1-az}  \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
\scriptstyle{0,d-1} & dz & 0 & 0 & \ldots & 0 & bz \\
\scriptstyle{*,0} & dz\frac{1}{1-az} & cz & acz^2 &
    \ldots & a^{d-2}cz^{d-1} & bz\frac{1}{1-az} \\
\end{block}
\end{blockarray}.
\end{equation*}

$H=(1,0,\ldots,0)$ and $T=(1+az+\ldots+(az)^{d-1}, \ldots, 1+az, 1,
1/(1-az))$.









%%%%%%%%%%%%%%%%%% Average number of errors %%%%%%%%%%%%%%%%%%%%%
\section{Average quantities}
\label{sec:av}

\subsection{General approach}
\label{sec:genapp}
Analytic combinatorics also allows computing the average of many
quantities of interest such as the average number of errors.  So far,
reads were characterized by a single quantity: their size. Now we need to
introduce a second quantity, namely the number of substitutions so we will
bring in a second variable. More specifically, if $a_{k,n}$ is the total
weight of seedless reads of size $k$ and with $n$ substitutions, the
average number of substitutions for reads of size $k$ is by definition


\begin{equation}
\label{eq:av}
\left( \sum_{n=0}^\infty na_{k,n} \right) \Big/
 \left( \sum_{n=0}^\infty a_{k,n} \right).
\end{equation}

To compute this quantity, we will manipulate bivariate generating
functions. We introduce the variable $u$ marking the number of
substitutions and we write

\begin{equation*}
S(z,u) = \sum_{k=0}^\infty\sum_{n=0}^\infty a_{k,n}z^ku^n.
\end{equation*}

Finding an explicit formula for $S(z,u)$ is the focus of
section~\ref{sec:avsub} .  For now, observe that (\ref{eq:av}) can be
expressed from the coefficients of $S(z,u)$. On the one hand, we have

\begin{equation*}
S(z,1) = \sum_{k=0}^\infty \left( \sum_{n=0}^\infty a_{k,n} \right) z^k.
\end{equation*}

So the denominator of (\ref{eq:av}) is the coefficient of $z^k$ in
$S(z,1)$. On the other hand, by taking the derivative of $S(z,u)$ with
respect to $u$, and setting $u=1$, we obtain

\begin{equation*}
\frac{\partial S(z,u)}{\partial u} \Bigr|_{\substack{\\u=1}} =
\sum_{k=0}^\infty \left( \sum_{n=0}^\infty na_{k,n} \right) z^k.
\end{equation*}

So the numerator of (\ref{eq:av}) is the coefficient of $z^k$ in
$S_u(z,1)$ (where $S_u$ is an alternative notation for the partial
derivative with respect to $u$).

Note that $S(z,1)$ and $S_u(z,1)$ are
actually univariate weighted generating functions, so we can use the
methods developed earlier to obtain asymptotic estimates for their
coefficients.

To use this approach, we need to find an explicit expression for $S(z,u)$,
which we onw do for the case of uniform substitutions.





\begin{proposition}
\label{th:ass2}
If a weighted generating function can be expressed as $P(z)/Q(z)^2$, where
$P$ and $Q$ are polynomials and the roots of $Q$ are simple, then the
coefficient of $z^k$ is asymptotically equivalent to

\begin{equation}
\label{eq:ass2}
\left( (k+1)\frac{P(z_1)}{z_1 Q'(z_1)^2} - \frac{P'(z_1)}{Q'(z_1)^2} +
\frac{P(z_1)Q''(z_1)}{Q'(z_1)^3} \right)
\frac{1}{z_1^{k+1}},
\end{equation}

\noindent
where $z_1$ is the root of $Q$ with smallest \textit{modulus}.
\end{proposition}

As in proposition~\ref{th:ass}, we start by proving the lemma that will
give the functional expression of the asymptotic expansion.

\begin{lemma}
\label{lemma:poles2}
For $|z| < a$ we have

\begin{equation}
\label{eq:poles2}
\frac{1}{(1-z/a)^2} = \sum_{k=0}^\infty (k+1)\frac{z^k}{a^k}.
\end{equation}
\end{lemma}

\begin{proof}
\begin{equation*}
\frac{1}{(1-z/a)^2} = a \left( \frac{1}{1-z/a} \right)'
= a \sum_{k=0}^\infty \frac{kz^{k-1}}{a^k},
\end{equation*}

\noindent
where the last equality is obtained by applying lemma~\ref{lemma:poles}
and differentiating.
\end{proof}

We now prove proposition~\ref{th:ass2}.

\begin{proof}
As in the proof of proposition~\ref{th:ass}, let $z_1, z_2, \ldots, z_n$
be the complex roots of $Q$, sorted by increasing order of
\textit{modulus}. Since the roots of $Q(z)^2$ all have multiplicity 2,
there exists constants $\alpha_1, \ldots, \alpha_n$ and $\beta_1, \ldots,
\beta_n$ such that the partial fraction decomposition of the rational
function $P(z)/Q(z)^2$ can be written as

\begin{equation*}
\frac{P(z)}{Q(z)^2} = 
\sum_{j=1}^n \frac{\alpha_j}{(z-z_j)^2} + \frac{\beta_j}{z-z_j} =
\sum_{j=1}^n \frac{\alpha_j/z_j^2}{(1-z/z_j)^2}
-\frac{\beta_j/z_j}{1-z/z_j}.
\end{equation*}

As in the proof of proposition~\ref{th:ass}, we assumed without loss of
generality that the degree of $P$ is lower than the degree of $Q$. Now
applying lemmas~\ref{lemma:poles} and \ref{lemma:poles2}, we see that
the coefficient of $z^k$ in $P(z)/Q(z)^2$ can be expressed as

\begin{equation}
\label{eq:fullass2}
\sum_{j=1}^n (k+1)\frac{\alpha_j}{z_j^{k+2}}-\frac{\beta_j}{z_j^{k+1}}.
\end{equation}

The sum above is dominated by the term with the highest exponential rate
of increase, \textit{i.e.} the first because $z_1$ has smallest modulus by
definition. Thus, the coefficient of $z^k$ in $P(z)/Q(z)^2$ is
asymptotically equivalent to 

\begin{equation*}
(k+1)\frac{\alpha_1}{z_1^{k+2}}-\frac{\beta_1}{z_1^{k+1}}.
\end{equation*}

We now need to find the values of $\alpha_1$ and $\beta_1$. As $z_1$ is a
root of $Q$, there exists a polynomial $Q_1(z)$ such that

\begin{equation}
\label{eq:Q1}
Q(z) = (z-z_1)Q_1(z).
\end{equation}

We can thus write $P(z)/Q(z)^2$ as

\begin{equation}
\label{eq:misc1}
\frac{P(z)}{(z-z_1)^2Q_1(z)^2} = \frac{\alpha_1}{(z-z_1)^2} +
\frac{\beta_1}{z-z_1} +
\sum_{j=2}^n \frac{\alpha_j}{(z-z_j)^2} + \frac{\beta_j}{z-z_j}.
\end{equation}

Multiplying both sides of (\ref{eq:misc1}) by $(z-z_1)^2$ and setting $z =
z_1$ shows that $\alpha_1 = P(z_1)/Q_1(z_1)^2$. To find the value of
$Q_1(z_1)$ we differentiate (\ref{eq:Q1}) and set $z = z_1$, yielding
$Q'(z_1) = Q_1(z_1)$. Finally we obtain $\alpha_1 = P(z_1) / Q'(z_1)^2$.

To find the value of $\beta_1$, we subtract $\alpha_1/(z-z_1)^2$ on both
sides of (\ref{eq:misc1}) and obtain

\begin{equation}
\label{eq:misc2}
\frac{P(z) - P(z_1)Q_1(z)^2/Q_1(z_1)^2}{(z-z_1)^2Q_1(z)^2} =
\frac{\beta_1}{z-z_1} + 
\sum_{j=2}^n \frac{\alpha_j}{(z-z_j)^2} + \frac{\beta_j}{z-z_j}.
\end{equation}

Since $P(z_1) - P(z_1)Q_1(z_1)^2/Q_1(z_1)^2 = 0$, there exists a
polynomial $Q_2(z)$ such that

\begin{equation}
\label{eq:Q2}
P(z) - P(z_1)Q_1(z)^2/Q_1(z_1)^2 = (z-z_1)Q_2(z).
\end{equation}


Multiplying (\ref{eq:misc2}) by $z-z_1$ and setting $z =
z_1$, we obtain $\beta_1 = Q_2(z_1)/Q_1(z_1)^2$. To find the value of
$Q_2(z_1)$ we differentiate (\ref{eq:Q1}) two times and (\ref{eq:Q2})
one time, finally obtaining
$\beta_1 = P'(z_1)/Q'(z_1)^2 - P(z_1)Q''(z_1)/Q'(z_1)^3$, which concludes
the proof.
\end{proof}


\begin{remark}
Expression (\ref{eq:ass2}) is asymptotically equivalent to the simpler
expression

\begin{equation}
\label{eq:ass3}
(k+1)\frac{P(z_1)}{Q'(z_1)^2}\frac{1}{z_1^{k+2}}.
\end{equation}

However, the convergence of expression (\ref{eq:ass3}) is slow. Indeed,
dividing the exact expression (\ref{eq:fullass2}) by (\ref{eq:ass3}) gives
an error term in $O(1/k)$. In comparison, dividing (\ref{eq:fullass2}) by
(\ref{eq:ass2}) gives an error term in $O(|z_1/z_2|^k)$, which decreases
exponentially, as in proposition~\ref{th:ass}. \end{remark}





\subsection{Substitutions}
\label{sec:avsub}

As we have seen several times, the analytic combinatorics approach is to
write the generating function of simple objects and then combine them into
more complex objects. In order to mark substitutions with the variable
$u$, we update their weighted generating function to $pzu$. As in
section~\ref{sec:substitutions}, $p$ is the probability of a substitution,
so it is the weight in this expression. For every subtitution, the powers
of $z$ and $u$ increase by $1$, and so do the size and the number of
substitutions of the read.

We could derive the weighted generating function by the transfer matrix
method described in section~\ref{sec:substitutions}, but we can get an
immediate result by using equation (\ref{eq:Sp}), where we observed
that the weighted generating function of seedless reads can be expressed
as

\begin{equation}
\tag{\ref{eq:Sp}}
\frac{1+F_d(z)}{1-pz(1+F_d(z))} =
(1+F_d(z)) \sum_{n=0}^\infty \big( pz(1+F_d(z)) \big)^n.
\end{equation}

Here the weighted generating function of substitutions appears explicitly
as $pz$. We can replace it by $pzu$ to obtain directly

\begin{equation}
\label{eq:Szu}
(1+F_d(z)) \sum_{n=0}^\infty \big( pzu(1+F_d(z)) \big)^n
= \frac{1+F_d(z)}{1-pzu\big( 1+F_d(z) \big)}.
\end{equation}


$S(z,1)$ is simply the weighted generating function of seedless reads
derived in section~\ref{sec:substitutions}, and for which we already
derived the coefficient asymptotics. So we already have the denominator of
(\ref{eq:av}). Now differentiating (\ref{eq:Szu}) with respect to $u$, we
obtain

\begin{equation}
\label{eq:dSdu}
\frac{\partial }{\partial u}
\left(\frac{1+F_d(z)}{1-pzu\big( 1+F_d(z) \big)}
\right) \Biggr|_{\substack{\\u=1}} = 
\frac{pz(1+F_d(z))^2}{\big( 1 - pz(1+F_d(z)) \big)^2}.
\end{equation}

Because of the form of the weighted generating function, we have to use
proposition~\ref{th:ass2}. The dominant singularity is the same for both
the numerator and the denominator, so the terms in $z_1^{k+1}$ cancel out.
What remains is an expression of the form $C_1k + C_2$. In other words,
the average number of substitutions in seedless reads increases as an
affine function of the size. We make this result more accurate in the
following proposition.


\begin{proposition}
\label{th:avsub}
Under the assumptions of the error model of
section~\ref{sec:substitutions}, the average number of substitutions in
seedless reads of size $k$ is asymptotically equivalent to

\begin{equation*}
\frac{(1-qz)(C_1(k+1) - C_2)}{1-(d+1-dqz_1)(qz_1)^d},
\end{equation*}

\noindent
with

\begin{gather*}
C_1 = 1-(qz_1)^d, \text{ and} \\
C_2 = \frac{
\big(1-(2-(1-qz_1)d^2+2d)(qz_1)^d+(1+(1-qz_1)d^2-2d)(qz_1)^{2d} \big)}
{1-(d+1-dqz_1)(qz_1)^d},
\end{gather*}

\noindent
and where $z_1$ is the only positive root of the polynomial
$1-pz(1+F_d(z))$.
\end{proposition}

\begin{proof}
Apply proposition~\ref{th:ass2} to (\ref{eq:dSdu}), use
proposition~\ref{th:p} and simplify.
\end{proof}

\begin{remark}
The estimate obtained from (\ref{eq:ass3}) instead of (\ref{eq:ass2})
increases linearly with $k$. In this case, only the relative error
vanishes asymptotically. The absolute error remains constant.
\end{remark}


\begin{figure}[h]
\centering
\includegraphics[scale=0.445]{simulp-average.pdf}
\caption{\textbf{Estimating the average number of substitutions}.
The average numbers of substitutions are shown for seedless reads of given
size, either estimated by 10,000,000 random simulations (dots), or by
analytic combinatorics (lines). The curves are drawn for $d=17$, $p=0.08$,
$p=0.10$ $p=0.12$ (from bottom to top). The difference never exceeds 0.004
on the examples shown here.}
\label{fig:simulavsub}
\end{figure}





\subsection{Substitutions and deletions}
\label{sec:avdel}

We now take the weighted generating function of the seedless reads in the
error model of section~\ref{sec:deletions}

\begin{equation}
\tag{\ref{eq:Sdel}}
\frac{1+(1-\delta)F_d(z)}
  {1-pz - \big(pz(1-\delta) + \delta\big)F_d(z)}.
\end{equation}

Here it is important to remember that for simplicity, equation
(\ref{eq:Sdel}) ignores the deletions that occur immediately before and
after substitutions, because they have no impact on the presence of a
seed. For the purpose of counting deletions, we need to take them all into
consideration, but this requires working with the proper weighted
generating function.

A read can be thought of as a walk on the graph shown in
figure~\ref{fig:deletions2}. Even though deletions appear as an explicit
state, their size is $0$ and they occur only between nucleotides (which
implies that a read can neither start nor end with a deletion). Also, a
deletion must be followed by either a match or a substitution; it cannot
be followed by a deletion. The reason is that a deletion has size $0$
regardless the number of deleted nucleotides. In the symbolic
representation of the read, either there is a deletion of any size between
two nucleotides, or there is no deletion.

\begin{figure}[h]
\centering
\includegraphics[scale=0.9]{deletions2.pdf}
\caption{\textbf{Error model with explicit deletions}. 
Substitution and deletion rates $p$ and $\delta$ are assumed to be
constant throughout the read. Deletions have a state of their own.
They can occur before or after a substitution.}
\label{fig:deletions2}
\end{figure}

In this error model, reads are sequences of three items:
error-free intervals withs symbol $\Delta_0$ and weighted generating
function $F(z)$, substitutions with symbol $S$ and weighted generating
function $pz$, and deletions with symbol $D$ and weighted generating
function $\delta$.

The only forbidden transitions in the sequence are that two error-free
intervals cannot follow each other (together they form a single error-free
interval) and two deletions cannot follow each other (together they form a
single deletion).

As in the previous section, in order to count deletions with parameter $v$
we update their weigted generating function to $\delta v$. The transfer
matrix then becomes a function of $z$, marking the size, and of $v$,
marking the deletions. The transfer matrix of the sequences is thus


\begin{equation*}
M(z,v) = 
\begin{blockarray}{cccc}
       & \smDELz & \smS & \smD \\
\begin{block}{c[ccc]}
\smDELz & 0              & (1-\delta)pz & \delta v \\
\smS    & (1-\delta)F(z) & (1-\delta)pz & \delta v \\
\smD    & F(z)           & pz           & 0        \\
\end{block}
\end{blockarray}.
\end{equation*}

Remember that reads can neither start nor end by a deletion because they
are undetectable at those positions. Thus the head vector is $H = (F(z),
pz, 0)$ and the tail vector is $T = (1,1,0)$. The weighted generating
function $1+H \cdot (I-M(z,v))^{-1} \cdot T$ has a cumbersome expression
not written here because we will not need it.

Following the general strategy to count average quantities, we
differentiate the weighted generating function with respect to $v$ and
set $v=1$ and obtain

\begin{equation}
\label{eq:full_deletions}
\frac{\delta\big( pz(1+(1-\delta)F(z)+F(z)) \big)^2}
{\big( 1-pz - \big(pz(1-\delta) + \delta\big)F_d(z) \big)^2}.
\end{equation}

As in the section~\ref{sec:avsub}, we derive the asymptotics by applying
proposition~\ref{th:ass2} to (\ref{eq:full_deletions}) and by applying
proposition~\ref{th:ass} to (\ref{eq:Sdel}). The terms $z_1^{k+1}$ cancel
out when taking the ratio as per equation (\ref{eq:av}), so the number of
deletions in seedless reads is asymptotically linear, \textit{i.e.} of the
form $C_1k+C_2$, where $C_1$ and $C_2$ are constants. In both cases, $C_1$
and $C_2$ have cumbersome expressions, but they are easy to compute from
propositions~\ref{th:ass} and \ref{th:ass2}.

\begin{figure}[h]
\centering
\includegraphics[scale=0.445]{simuldel-average2.pdf}
\caption{\textbf{Example estimates of average number of deletions}. The
average numbers of deletions are shown for seedless reads of given size,
either estimated by 10,000,000 random simulations (dots), or by analytinc
combinatorics (lines). The curves are drawn for $d=17$, $p=0.05$, and
$\delta=0.14$, $\delta=0.15$ or $\delta=0.16$ (from bottom to top). The
difference never exceeds 0.003 on the examples shown here.}
\label{fig:simulavdel}
\end{figure}




\subsection{Substitutions, deletions and insertions}

Finally, we take the weighted generating function of the seedless reads in
the error model of section~\ref{sec:insertions}

\begin{equation}
\tag{\ref{eq:Sindel}}
\frac{(1-r)\big( 1-(\tilde{r}-r)z \big) \left(1+(1-\delta)F_d(z)
\right)}{1-a(z)-b(z)F_d(z)}.
\end{equation}

\section{Discussion}

Some discussion goes here.

%---------------------------------------------------------------
%---------------------------------------------------------------

\bibliography{pubmed,extra}
\bibliographystyle{plain}

%----------------------------------------------------------------

\end{document}

%gs -dNoOutputFonts -sDEVICE=pdfwrite -o out.pdf latex.pdf 
